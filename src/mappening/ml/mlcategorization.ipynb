{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "nbpresent": {
     "id": "92f7983f-ec8a-41dd-987a-913b1754d3a4"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jfuentes/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /Users/jfuentes/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /Users/jfuentes/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/jfuentes/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jfuentes/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('perluniprops')\n",
    "nltk.download('nonbreaking_prefixes')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "e9a9474a-3d31-4794-be29-34bad530e8ea"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening the secret door...\n",
      "Got the .env secrets...\n",
      "Connected to database!\n",
      "Got database collections...\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /Users/jfuentes/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /Users/jfuentes/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jfuentes/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/jfuentes/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Login manager set up with auth Blueprint!\n"
     ]
    }
   ],
   "source": [
    "##Needed to get access to mappening.utils.database since this is under mappening.ml\n",
    "import sys\n",
    "sys.path.insert(0,'./../..')\n",
    "\n",
    "from mappening.utils.database import ucla_events_collection, events_ml_collection\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from nltk.tokenize.moses import MosesTokenizer\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from gensim.models import Phrases\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "b9cffde6-aea9-4d79-914e-6958234c679b"
    }
   },
   "outputs": [],
   "source": [
    "class PreprocessText:\n",
    "    def __init__(self, categorizedEvents):\n",
    "        \"\"\"categorizedEvents should be a list of dictionaries each corresponding to an event\n",
    "            X is the tokenized preprocessed text\n",
    "            Y is the corresponding categories\n",
    "            phraseMl is the phrase model that can further trained and used\n",
    "            phrases is a list of all the phrases identified\"\"\"\n",
    "        self.X = []\n",
    "        self.Y = []\n",
    "        tokenizer = MosesTokenizer() #tokenizers are basically an advanced split\n",
    "        for e in categorizedEvents:\n",
    "            text = e[u'name'] + \" \" + e[u'description']\n",
    "            text = tokenizer.tokenize(text)\n",
    "            text = self.preprocess(text)\n",
    "            self.X.append(text)\n",
    "            self.Y.append(e[u'category'])\n",
    "        self.phraseMl = Phrases(self.X, min_count=3) #learn ml model for phrase\n",
    "        self.X = list(self.phraseMl[self.X]) #use ml model for phrases\n",
    "#         self.X = list(self.phraseMl[self.X]) #get triples\n",
    "        self.phrases = phrases = set([w for doc in self.X for w in doc if '_' in w])\n",
    "        \n",
    "    def matchNotX(self, strg, search=re.compile(r'[^!#$%&()*+,-./:;<=>?@\\\\^_}|{~0123456789]').search):\n",
    "        \"\"\"make sure word has something than punctuation\"\"\"\n",
    "        return bool(search(strg)) #make sure word has something other than punctuation\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Remove all useless words and punct, make lowercase\"\"\"\n",
    "        stoplist = set('for a of the and to in . / '.split())\n",
    "        stoplist = set(nltk.corpus.stopwords.words('english')) | stoplist | set(string.punctuation)\n",
    "        return [word.strip(string.punctuation).lower() for word in text if word not in stoplist and self.matchNotX(word)]    \n",
    "        \n",
    "    def topBigrams(self, texts, n, tri=False):\n",
    "        \"\"\"Other method of getting phrases, currently unused because phrases can be further trained(online) and saved\"\"\"\n",
    "        flatTexts = []\n",
    "        for text in texts:\n",
    "            for word in text:\n",
    "                flatTexts.append(word)\n",
    "        bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "        trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "        topAnswers = []\n",
    "        if tri:\n",
    "            finder = nltk.collocations.TrigramCollocationFinder.from_words(flatTexts)\n",
    "            finder.apply_freq_filter(7)\n",
    "            return finder.nbest(trigram_measures.pmi, n)\n",
    "        else:\n",
    "            finder = nltk.collocations.BigramCollocationFinder.from_words(flatTexts)\n",
    "            finder.apply_freq_filter(7)\n",
    "            return finder.nbest(bigram_measures.pmi, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "90872305-5f7a-4d08-98bc-acd0c8198395"
    }
   },
   "outputs": [],
   "source": [
    "def gatherCategorizedEvents():\n",
    "    \"\"\"Return panda dataframe of events with category, description, and name\"\"\"\n",
    "    allCategorizedEvents = []\n",
    "    allEvents = events_ml_collection.find({}, {\"category\": 1, \"description\": 1, \"name\": 1, \"hoster\": 1, \"_id\": 0})\n",
    "    count = 0\n",
    "    for e in allEvents:\n",
    "        count += 1\n",
    "        e['hoster'] = e['hoster']['name']\n",
    "        if 'category' in e and 'description' in e and 'name' in e:\n",
    "            allCategorizedEvents.append(e)\n",
    "    modernEvents = reduceCategories(allCategorizedEvents)\n",
    "    print count, \"total events, learning from the\", len(modernEvents), \"well categorized events\"\n",
    "    return pd.DataFrame(modernEvents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def someCurrentCategories():\n",
    "    \"\"\"Looks at current events for the categories list, to be used if Facebook changes its events in the future\"\"\"\n",
    "    allCategorizedEvents = []\n",
    "    allEvents = ucla_events_collection.find({}, {\"category\": 1, \"description\": 1, \"name\": 1, \"_id\": 0})\n",
    "    for e in allEvents:\n",
    "        if 'category' in e and 'description' in e and 'name' in e:\n",
    "            allCategorizedEvents.append(e)\n",
    "    skTarget = [e['category'] for e in allCategorizedEvents]\n",
    "    count = sorted(list(set(skTarget)))\n",
    "    print(count)\n",
    "    \n",
    "curListOfCategories = [u'ART', u'CAUSE', u'COMEDY_PERFORMANCE', u'DANCE', u'DRINKS', u'FILM', u'FITNESS', u'FOOD',\n",
    "                       u'GAMES', u'GARDENING', u'HEALTH', u'LITERATURE', u'MEETUP', u'MUSIC', u'NETWORKING', u'PARTY',\n",
    "                       u'RELIGION', u'SHOPPING', u'SPORTS', u'THEATER', u'WELLNESS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduceCategories(events):\n",
    "    \"\"\"OTHER will be discarded from the training data\"\"\"\n",
    "    categoryMapping = {\n",
    "        u'BOOK': u'LITERATURE',\n",
    "        u'COMEDY': u'COMEDY_PERFORMANCE',\n",
    "        u'CLASS': u'OTHER',\n",
    "        u'DINING': u'FOOD',\n",
    "        u'FAMILY': u'OTHER',\n",
    "        u'FESTIVAL': u'PARTY',\n",
    "        u'FOOD_TASTING': u'FOOD',\n",
    "        u'FUNDRAISER': u'CAUSE',\n",
    "        u'LECTURE': u'OTHER',\n",
    "        u'MOVIE': u'FILM',\n",
    "        u'NEIGHBORHOOD': u'OTHER',\n",
    "        u'NIGHTLIFE': u'OTHER',\n",
    "        u'RELIGIOUS': u'RELIGION',\n",
    "        u'VOLUNTEERING': u'CAUSE',\n",
    "        u'WORKSHOP': u'OTHER'\n",
    "    }\n",
    "    \n",
    "    for e in events:\n",
    "        category = e['category']\n",
    "        if category in categoryMapping:\n",
    "            e['category'] = categoryMapping[category]\n",
    "    reducedEvents = [e for e in events if e['category'] != u'OTHER']\n",
    "    return reducedEvents\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbpresent": {
     "id": "fb083146-ae02-494f-9da7-8ae1a483ef92"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6936 total events, learning from the 2828 well categorized events\n"
     ]
    }
   ],
   "source": [
    "X = gatherCategorizedEvents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/ml/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "def train(classifier, X, y, trails=25):\n",
    "    scores = np.zeros(trails)\n",
    "    for i in tqdm(range(0, trails)):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=i)\n",
    "\n",
    "        classifier.fit(X_train, y_train)\n",
    "        scores[i] = (classifier.score(X_test, y_test))\n",
    "    print \"Average Accuracy over %d trials: %s\" % (trails, np.mean(scores))\n",
    "    classifier.fit(X, y)\n",
    "    return classifier\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictList(vectorizer, classifier, x):\n",
    "    print(x)\n",
    "    x = vectorizer.transform(x)\n",
    "    y_pred = classifier.predict(x)\n",
    "    print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def giveProbPerCategory(vectorizer, classifier, x, threshold=.15):\n",
    "    print(x)\n",
    "    x = vectorizer.transform(x)\n",
    "    y_pred = classifier.predict_proba(x)\n",
    "\n",
    "    strongest_category = ''\n",
    "    highest_match = 0\n",
    "    above_threshold = []\n",
    "    for i in range(len(classifier.classes_)):\n",
    "        \n",
    "        if y_pred[0][i] > highest_match:\n",
    "            highest_match = y_pred[0][i]\n",
    "            strongest_category = classifier.classes_[i]\n",
    "            \n",
    "        if y_pred[0][i] > threshold:\n",
    "            above_threshold.append(classifier.classes_[i])\n",
    "    \n",
    "    if not above_threshold:\n",
    "        return [strongest_category]\n",
    "    else:\n",
    "        return above_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbpresent": {
     "id": "2ce54c9a-1a7a-4d84-9972-922c9aa4db15"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 55.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy over 25 trials: 0.5812164073550212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    " \n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# tokenize and build vocab\n",
    "X_name_transform = vectorizer.fit_transform(X['name'])\n",
    "X_details_transform = vectorizer.fit_transform(X['description'])\n",
    "# print(vectorizer.vocabulary_)\n",
    "# print(vectorizer.idf_)\n",
    "\n",
    "nbModel = MultinomialNB()\n",
    "nbModel = train(nbModel, X_name_transform, X['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMBINE NAME AND DESCRIPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:01<00:00, 21.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy over 25 trials: 0.5854596888260254\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    " \n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# tokenize and build vocab\n",
    "X_name_transform = vectorizer.fit_transform(X['name'])\n",
    "X_details_transform = vectorizer.fit_transform(X['description'])\n",
    "X_total_transform = hstack([X_name_transform, X_details_transform])\n",
    "\n",
    "nbModel = MultinomialNB()\n",
    "nbModel = train(nbModel, X_total_transform, X['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.023214</td>\n",
       "      <td>0.005026</td>\n",
       "      <td>0.696605</td>\n",
       "      <td>0.981968</td>\n",
       "      <td>0.04</td>\n",
       "      <td>{u'alpha': 0.04}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.692955</td>\n",
       "      <td>0.981353</td>\n",
       "      <td>0.721868</td>\n",
       "      <td>0.984624</td>\n",
       "      <td>0.674866</td>\n",
       "      <td>0.979926</td>\n",
       "      <td>0.002720</td>\n",
       "      <td>0.000722</td>\n",
       "      <td>0.019321</td>\n",
       "      <td>0.001966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.025697</td>\n",
       "      <td>0.005325</td>\n",
       "      <td>0.693069</td>\n",
       "      <td>0.974367</td>\n",
       "      <td>0.06</td>\n",
       "      <td>{u'alpha': 0.06}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.691903</td>\n",
       "      <td>0.974427</td>\n",
       "      <td>0.721868</td>\n",
       "      <td>0.977200</td>\n",
       "      <td>0.665241</td>\n",
       "      <td>0.971474</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.023082</td>\n",
       "      <td>0.002338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "1       0.023214         0.005026         0.696605          0.981968   \n",
       "3       0.025697         0.005325         0.693069          0.974367   \n",
       "\n",
       "  param_alpha            params  rank_test_score  split0_test_score  \\\n",
       "1        0.04  {u'alpha': 0.04}                1           0.692955   \n",
       "3        0.06  {u'alpha': 0.06}                2           0.691903   \n",
       "\n",
       "   split0_train_score  split1_test_score  split1_train_score  \\\n",
       "1            0.981353           0.721868            0.984624   \n",
       "3            0.974427           0.721868            0.977200   \n",
       "\n",
       "   split2_test_score  split2_train_score  std_fit_time  std_score_time  \\\n",
       "1           0.674866            0.979926      0.002720        0.000722   \n",
       "3           0.665241            0.971474      0.000694        0.000490   \n",
       "\n",
       "   std_test_score  std_train_score  \n",
       "1        0.019321         0.001966  \n",
       "3        0.023082         0.002338  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# tokenize and build vocab\n",
    "X_name_transform = vectorizer.fit_transform(X['name'])\n",
    "X_details_transform = vectorizer.fit_transform(X['description'])\n",
    "X_total_transform = hstack([X_name_transform, X_details_transform])\n",
    "\n",
    "nbModel = MultinomialNB()\n",
    "param = {'alpha': [.01, .04, .05, .06, .3, .9, 1.5]}\n",
    "gs = GridSearchCV(nbModel, param)\n",
    "gs.fit(X_total_transform, X['category'])\n",
    "pd.DataFrame(gs.cv_results_).sort_values('mean_test_score', ascending=False)[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STOPLIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.025976</td>\n",
       "      <td>0.005302</td>\n",
       "      <td>0.706860</td>\n",
       "      <td>0.985860</td>\n",
       "      <td>0.04</td>\n",
       "      <td>{u'alpha': 0.04}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.706625</td>\n",
       "      <td>0.986148</td>\n",
       "      <td>0.729299</td>\n",
       "      <td>0.988865</td>\n",
       "      <td>0.684492</td>\n",
       "      <td>0.982567</td>\n",
       "      <td>0.001962</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>0.018253</td>\n",
       "      <td>0.002579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.029144</td>\n",
       "      <td>0.005285</td>\n",
       "      <td>0.703678</td>\n",
       "      <td>0.984091</td>\n",
       "      <td>0.05</td>\n",
       "      <td>{u'alpha': 0.05}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.702419</td>\n",
       "      <td>0.984017</td>\n",
       "      <td>0.726115</td>\n",
       "      <td>0.987275</td>\n",
       "      <td>0.682353</td>\n",
       "      <td>0.980983</td>\n",
       "      <td>0.001082</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.017849</td>\n",
       "      <td>0.002569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "1       0.025976         0.005302         0.706860          0.985860   \n",
       "2       0.029144         0.005285         0.703678          0.984091   \n",
       "\n",
       "  param_alpha            params  rank_test_score  split0_test_score  \\\n",
       "1        0.04  {u'alpha': 0.04}                1           0.706625   \n",
       "2        0.05  {u'alpha': 0.05}                2           0.702419   \n",
       "\n",
       "   split0_train_score  split1_test_score  split1_train_score  \\\n",
       "1            0.986148           0.729299            0.988865   \n",
       "2            0.984017           0.726115            0.987275   \n",
       "\n",
       "   split2_test_score  split2_train_score  std_fit_time  std_score_time  \\\n",
       "1           0.684492            0.982567      0.001962        0.001010   \n",
       "2           0.682353            0.980983      0.001082        0.000526   \n",
       "\n",
       "   std_test_score  std_train_score  \n",
       "1        0.018253         0.002579  \n",
       "2        0.017849         0.002569  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# tokenize and build vocab\n",
    "X_name_transform = vectorizer.fit_transform(X['name'])\n",
    "X_details_transform = vectorizer.fit_transform(X['description'])\n",
    "X_total_transform = hstack([X_name_transform, X_details_transform])\n",
    "\n",
    "nbModel = MultinomialNB()\n",
    "param = {'alpha': [.01, .04, .05, .06, .3, .9, 1.5]}\n",
    "gs = GridSearchCV(nbModel, param)\n",
    "gs.fit(X_total_transform, X['category'])\n",
    "pd.DataFrame(gs.cv_results_).sort_values('mean_test_score', ascending=False)[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.036891</td>\n",
       "      <td>0.006079</td>\n",
       "      <td>0.689887</td>\n",
       "      <td>0.993462</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{u'alpha': 0.01}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.680336</td>\n",
       "      <td>0.994140</td>\n",
       "      <td>0.721868</td>\n",
       "      <td>0.994698</td>\n",
       "      <td>0.667380</td>\n",
       "      <td>0.991548</td>\n",
       "      <td>0.005828</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.023213</td>\n",
       "      <td>0.001372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.029470</td>\n",
       "      <td>0.006869</td>\n",
       "      <td>0.686704</td>\n",
       "      <td>0.983558</td>\n",
       "      <td>0.04</td>\n",
       "      <td>{u'alpha': 0.04}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.689800</td>\n",
       "      <td>0.982952</td>\n",
       "      <td>0.715499</td>\n",
       "      <td>0.985684</td>\n",
       "      <td>0.654545</td>\n",
       "      <td>0.982039</td>\n",
       "      <td>0.000835</td>\n",
       "      <td>0.001367</td>\n",
       "      <td>0.024927</td>\n",
       "      <td>0.001549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0       0.036891         0.006079         0.689887          0.993462   \n",
       "1       0.029470         0.006869         0.686704          0.983558   \n",
       "\n",
       "  param_alpha            params  rank_test_score  split0_test_score  \\\n",
       "0        0.01  {u'alpha': 0.01}                1           0.680336   \n",
       "1        0.04  {u'alpha': 0.04}                2           0.689800   \n",
       "\n",
       "   split0_train_score  split1_test_score  split1_train_score  \\\n",
       "0            0.994140           0.721868            0.994698   \n",
       "1            0.982952           0.715499            0.985684   \n",
       "\n",
       "   split2_test_score  split2_train_score  std_fit_time  std_score_time  \\\n",
       "0           0.667380            0.991548      0.005828        0.000244   \n",
       "1           0.654545            0.982039      0.000835        0.001367   \n",
       "\n",
       "   std_test_score  std_train_score  \n",
       "0        0.023213         0.001372  \n",
       "1        0.024927         0.001549  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from nltk import WordNetLemmatizer, word_tokenize\n",
    "\n",
    "def lemma_tokenizer(text):\n",
    "    wn = WordNetLemmatizer()\n",
    "    tokenizer = word_tokenize\n",
    "    return [wn.lemmatize(w) for w in tokenizer(text)]\n",
    "\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer(stop_words='english', analyzer=lemma_tokenizer)\n",
    "\n",
    "# tokenize and build vocab\n",
    "X_name_transform = vectorizer.fit_transform(X['name'])\n",
    "X_details_transform = vectorizer.fit_transform(X['description'])\n",
    "X_total_transform = hstack([X_name_transform, X_details_transform])\n",
    "\n",
    "nbModel = MultinomialNB()\n",
    "param = {'alpha': [.01, .04, .05, .06, .3, .9, 1.5]}\n",
    "gs = GridSearchCV(nbModel, param)\n",
    "gs.fit(X_total_transform, X['category'])\n",
    "pd.DataFrame(gs.cv_results_).sort_values('mean_test_score', ascending=False)[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NGrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 3/25 [00:00<00:00, 23.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 26.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy over 25 trials: 0.7342008486562942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:03<00:00,  6.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy over 25 trials: 0.7344837340876944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:06<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy over 25 trials: 0.7327864214992929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/25 [00:00<00:03,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:02<00:00,  8.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy over 25 trials: 0.6703253182461104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:05<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy over 25 trials: 0.6511456859971712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/25 [00:00<00:03,  6.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:03<00:00,  7.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy over 25 trials: 0.5723338048090523\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "for ngrams in [(1,1), (1,2), (1,3), (2,2), (2,3), (3,3)]:\n",
    "    # create the transform\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', ngram_range=ngrams)\n",
    "\n",
    "    # tokenize and build vocab\n",
    "    X_name_transform = vectorizer.fit_transform(X['name'])\n",
    "    X_details_transform = vectorizer.fit_transform(X['description'])\n",
    "    X_total_transform = hstack([X_name_transform, X_details_transform])\n",
    "\n",
    "    nbModel = MultinomialNB(alpha=.04)\n",
    "    print(ngrams)\n",
    "    nbModel = train(nbModel, X_total_transform, X['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/ml/lib/python2.7/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/anaconda2/envs/ml/lib/python2.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda2/envs/ml/lib/python2.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>43.664493</td>\n",
       "      <td>0.433965</td>\n",
       "      <td>0.708274</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>None</td>\n",
       "      <td>300</td>\n",
       "      <td>{u'n_estimators': 300, u'max_depth': None}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.706294</td>\n",
       "      <td>0.999113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.710247</td>\n",
       "      <td>0.999558</td>\n",
       "      <td>0.733570</td>\n",
       "      <td>0.999558</td>\n",
       "      <td>0.666068</td>\n",
       "      <td>0.999119</td>\n",
       "      <td>5.691373</td>\n",
       "      <td>0.131625</td>\n",
       "      <td>0.023093</td>\n",
       "      <td>0.000217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>23.899056</td>\n",
       "      <td>0.275176</td>\n",
       "      <td>0.703324</td>\n",
       "      <td>0.998408</td>\n",
       "      <td>90</td>\n",
       "      <td>150</td>\n",
       "      <td>{u'n_estimators': 150, u'max_depth': 90}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.998227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.704947</td>\n",
       "      <td>0.998232</td>\n",
       "      <td>0.717584</td>\n",
       "      <td>0.999558</td>\n",
       "      <td>0.676840</td>\n",
       "      <td>0.998239</td>\n",
       "      <td>1.093417</td>\n",
       "      <td>0.031485</td>\n",
       "      <td>0.017164</td>\n",
       "      <td>0.000601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>29.502144</td>\n",
       "      <td>0.272200</td>\n",
       "      <td>0.703324</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>None</td>\n",
       "      <td>150</td>\n",
       "      <td>{u'n_estimators': 150, u'max_depth': None}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.690559</td>\n",
       "      <td>0.999113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.706714</td>\n",
       "      <td>0.999558</td>\n",
       "      <td>0.728242</td>\n",
       "      <td>0.999558</td>\n",
       "      <td>0.667864</td>\n",
       "      <td>0.999119</td>\n",
       "      <td>2.703972</td>\n",
       "      <td>0.070401</td>\n",
       "      <td>0.021978</td>\n",
       "      <td>0.000217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>46.596309</td>\n",
       "      <td>0.501277</td>\n",
       "      <td>0.701202</td>\n",
       "      <td>0.998319</td>\n",
       "      <td>90</td>\n",
       "      <td>300</td>\n",
       "      <td>{u'n_estimators': 300, u'max_depth': 90}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.694056</td>\n",
       "      <td>0.998227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.701413</td>\n",
       "      <td>0.997790</td>\n",
       "      <td>0.722913</td>\n",
       "      <td>0.999558</td>\n",
       "      <td>0.658887</td>\n",
       "      <td>0.998679</td>\n",
       "      <td>2.945843</td>\n",
       "      <td>0.061167</td>\n",
       "      <td>0.024543</td>\n",
       "      <td>0.000763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32.256487</td>\n",
       "      <td>0.457183</td>\n",
       "      <td>0.691655</td>\n",
       "      <td>0.990275</td>\n",
       "      <td>60</td>\n",
       "      <td>300</td>\n",
       "      <td>{u'n_estimators': 300, u'max_depth': 60}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.685315</td>\n",
       "      <td>0.990248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.704947</td>\n",
       "      <td>0.988064</td>\n",
       "      <td>0.706927</td>\n",
       "      <td>0.992494</td>\n",
       "      <td>0.653501</td>\n",
       "      <td>0.990753</td>\n",
       "      <td>4.976321</td>\n",
       "      <td>0.147348</td>\n",
       "      <td>0.020596</td>\n",
       "      <td>0.001432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "11      43.664493         0.433965         0.708274          0.999293   \n",
       "7       23.899056         0.275176         0.703324          0.998408   \n",
       "10      29.502144         0.272200         0.703324          0.999293   \n",
       "8       46.596309         0.501277         0.701202          0.998319   \n",
       "5       32.256487         0.457183         0.691655          0.990275   \n",
       "\n",
       "   param_max_depth param_n_estimators  \\\n",
       "11            None                300   \n",
       "7               90                150   \n",
       "10            None                150   \n",
       "8               90                300   \n",
       "5               60                300   \n",
       "\n",
       "                                        params  rank_test_score  \\\n",
       "11  {u'n_estimators': 300, u'max_depth': None}                1   \n",
       "7     {u'n_estimators': 150, u'max_depth': 90}                2   \n",
       "10  {u'n_estimators': 150, u'max_depth': None}                2   \n",
       "8     {u'n_estimators': 300, u'max_depth': 90}                4   \n",
       "5     {u'n_estimators': 300, u'max_depth': 60}                5   \n",
       "\n",
       "    split0_test_score  split0_train_score       ...         split2_test_score  \\\n",
       "11           0.706294            0.999113       ...                  0.710247   \n",
       "7            0.692308            0.998227       ...                  0.704947   \n",
       "10           0.690559            0.999113       ...                  0.706714   \n",
       "8            0.694056            0.998227       ...                  0.701413   \n",
       "5            0.685315            0.990248       ...                  0.704947   \n",
       "\n",
       "    split2_train_score  split3_test_score  split3_train_score  \\\n",
       "11            0.999558           0.733570            0.999558   \n",
       "7             0.998232           0.717584            0.999558   \n",
       "10            0.999558           0.728242            0.999558   \n",
       "8             0.997790           0.722913            0.999558   \n",
       "5             0.988064           0.706927            0.992494   \n",
       "\n",
       "    split4_test_score  split4_train_score  std_fit_time  std_score_time  \\\n",
       "11           0.666068            0.999119      5.691373        0.131625   \n",
       "7            0.676840            0.998239      1.093417        0.031485   \n",
       "10           0.667864            0.999119      2.703972        0.070401   \n",
       "8            0.658887            0.998679      2.945843        0.061167   \n",
       "5            0.653501            0.990753      4.976321        0.147348   \n",
       "\n",
       "    std_test_score  std_train_score  \n",
       "11        0.023093         0.000217  \n",
       "7         0.017164         0.000601  \n",
       "10        0.021978         0.000217  \n",
       "8         0.024543         0.000763  \n",
       "5         0.020596         0.001432  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# tokenize and build vocab\n",
    "X_name_transform = vectorizer.fit_transform(X['name'])\n",
    "X_details_transform = vectorizer.fit_transform(X['description'])\n",
    "X_total_transform = hstack([X_name_transform, X_details_transform])\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "param = {'n_estimators': [10, 150, 300],\n",
    "          'max_depth': [30,60,90, None]}\n",
    "gs = GridSearchCV(rf, param, cv=5, n_jobs=-1)\n",
    "gs.fit(X_total_transform, X['category'])\n",
    "pd.DataFrame(gs.cv_results_).sort_values('mean_test_score', ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4616.345689</td>\n",
       "      <td>0.062979</td>\n",
       "      <td>0.682461</td>\n",
       "      <td>0.999204</td>\n",
       "      <td>150</td>\n",
       "      <td>{u'n_estimators': 150}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.999113</td>\n",
       "      <td>0.691228</td>\n",
       "      <td>...</td>\n",
       "      <td>0.696113</td>\n",
       "      <td>0.999558</td>\n",
       "      <td>0.696270</td>\n",
       "      <td>0.999558</td>\n",
       "      <td>0.646320</td>\n",
       "      <td>0.999119</td>\n",
       "      <td>2130.005668</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>0.018658</td>\n",
       "      <td>0.000332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1154.908539</td>\n",
       "      <td>0.116251</td>\n",
       "      <td>0.682107</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>300</td>\n",
       "      <td>{u'n_estimators': 300}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.687063</td>\n",
       "      <td>0.999113</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>...</td>\n",
       "      <td>0.694346</td>\n",
       "      <td>0.999558</td>\n",
       "      <td>0.687389</td>\n",
       "      <td>0.999558</td>\n",
       "      <td>0.639138</td>\n",
       "      <td>0.999119</td>\n",
       "      <td>585.401769</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>0.021953</td>\n",
       "      <td>0.000217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>96.969431</td>\n",
       "      <td>0.041524</td>\n",
       "      <td>0.631188</td>\n",
       "      <td>0.839109</td>\n",
       "      <td>10</td>\n",
       "      <td>{u'n_estimators': 10}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.627622</td>\n",
       "      <td>0.836436</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>...</td>\n",
       "      <td>0.637809</td>\n",
       "      <td>0.840407</td>\n",
       "      <td>0.653641</td>\n",
       "      <td>0.834879</td>\n",
       "      <td>0.578097</td>\n",
       "      <td>0.840159</td>\n",
       "      <td>9.001699</td>\n",
       "      <td>0.014812</td>\n",
       "      <td>0.028472</td>\n",
       "      <td>0.003118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "1    4616.345689         0.062979         0.682461          0.999204   \n",
       "2    1154.908539         0.116251         0.682107          0.999293   \n",
       "0      96.969431         0.041524         0.631188          0.839109   \n",
       "\n",
       "  param_n_estimators                  params  rank_test_score  \\\n",
       "1                150  {u'n_estimators': 150}                1   \n",
       "2                300  {u'n_estimators': 300}                2   \n",
       "0                 10   {u'n_estimators': 10}                3   \n",
       "\n",
       "   split0_test_score  split0_train_score  split1_test_score       ...         \\\n",
       "1           0.681818            0.999113           0.691228       ...          \n",
       "2           0.687063            0.999113           0.701754       ...          \n",
       "0           0.627622            0.836436           0.657895       ...          \n",
       "\n",
       "   split2_test_score  split2_train_score  split3_test_score  \\\n",
       "1           0.696113            0.999558           0.696270   \n",
       "2           0.694346            0.999558           0.687389   \n",
       "0           0.637809            0.840407           0.653641   \n",
       "\n",
       "   split3_train_score  split4_test_score  split4_train_score  std_fit_time  \\\n",
       "1            0.999558           0.646320            0.999119   2130.005668   \n",
       "2            0.999558           0.639138            0.999119    585.401769   \n",
       "0            0.834879           0.578097            0.840159      9.001699   \n",
       "\n",
       "   std_score_time  std_test_score  std_train_score  \n",
       "1        0.010900        0.018658         0.000332  \n",
       "2        0.018500        0.021953         0.000217  \n",
       "0        0.014812        0.028472         0.003118  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# tokenize and build vocab\n",
    "X_name_transform = vectorizer.fit_transform(X['name'])\n",
    "X_details_transform = vectorizer.fit_transform(X['description'])\n",
    "X_total_transform = hstack([X_name_transform, X_details_transform])\n",
    "\n",
    "gb = GradientBoostingClassifier()\n",
    "param = {'n_estimators': [10, 150, 300]}\n",
    "gs = GridSearchCV(gb, param, cv=5, n_jobs=-1)\n",
    "gs.fit(X_total_transform, X['category'])\n",
    "pd.DataFrame(gs.cv_results_).sort_values('mean_test_score', ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = vectorizer.build_tokenizer()\n",
    "\n",
    "for w in vectorizer.vocabulary_:\n",
    "    if not matchNotX(w):\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(set(skTarget)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "someCurrentCategories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
