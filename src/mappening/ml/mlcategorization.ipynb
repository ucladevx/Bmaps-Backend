{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "92f7983f-ec8a-41dd-987a-913b1754d3a4"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jfuentes/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /Users/jfuentes/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /Users/jfuentes/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/jfuentes/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('perluniprops')\n",
    "nltk.download('nonbreaking_prefixes')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "nbpresent": {
     "id": "e9a9474a-3d31-4794-be29-34bad530e8ea"
    }
   },
   "outputs": [],
   "source": [
    "##Needed to get access to mappening.utils.database since this is under mappening.ml\n",
    "import sys\n",
    "sys.path.insert(0,'./../..')\n",
    "\n",
    "from mappening.utils.database import ucla_events_collection, events_ml_collection\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from nltk.tokenize.moses import MosesTokenizer\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from gensim.models import Phrases\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "b9cffde6-aea9-4d79-914e-6958234c679b"
    }
   },
   "outputs": [],
   "source": [
    "class PreprocessText:\n",
    "    def __init__(self, categorizedEvents):\n",
    "        \"\"\"categorizedEvents should be a list of dictionaries each corresponding to an event\n",
    "            X is the tokenized preprocessed text\n",
    "            Y is the corresponding categories\n",
    "            phraseMl is the phrase model that can further trained and used\n",
    "            phrases is a list of all the phrases identified\"\"\"\n",
    "        self.X = []\n",
    "        self.Y = []\n",
    "        tokenizer = MosesTokenizer() #tokenizers are basically an advanced split\n",
    "        for e in categorizedEvents:\n",
    "            text = e[u'name'] + \" \" + e[u'description']\n",
    "            text = tokenizer.tokenize(text)\n",
    "            text = self.preprocess(text)\n",
    "            self.X.append(text)\n",
    "            self.Y.append(e[u'category'])\n",
    "        self.phraseMl = Phrases(self.X, min_count=3) #learn ml model for phrase\n",
    "        self.X = list(self.phraseMl[self.X]) #use ml model for phrases\n",
    "#         self.X = list(self.phraseMl[self.X]) #get triples\n",
    "        self.phrases = phrases = set([w for doc in self.X for w in doc if '_' in w])\n",
    "        \n",
    "    def matchNotX(self, strg, search=re.compile(r'[^!#$%&()*+,-./:;<=>?@\\\\^_}|{~0123456789]').search):\n",
    "        \"\"\"make sure word has something than punctuation\"\"\"\n",
    "        return bool(search(strg)) #make sure word has something other than punctuation\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Remove all useless words and punct, make lowercase\"\"\"\n",
    "        stoplist = set('for a of the and to in . / '.split())\n",
    "        stoplist = set(nltk.corpus.stopwords.words('english')) | stoplist | set(string.punctuation)\n",
    "        return [word.strip(string.punctuation).lower() for word in text if word not in stoplist and self.matchNotX(word)]    \n",
    "        \n",
    "    def topBigrams(self, texts, n, tri=False):\n",
    "        \"\"\"Other method of getting phrases, currently unused because phrases can be further trained(online) and saved\"\"\"\n",
    "        flatTexts = []\n",
    "        for text in texts:\n",
    "            for word in text:\n",
    "                flatTexts.append(word)\n",
    "        bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "        trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "        topAnswers = []\n",
    "        if tri:\n",
    "            finder = nltk.collocations.TrigramCollocationFinder.from_words(flatTexts)\n",
    "            finder.apply_freq_filter(7)\n",
    "            return finder.nbest(trigram_measures.pmi, n)\n",
    "        else:\n",
    "            finder = nltk.collocations.BigramCollocationFinder.from_words(flatTexts)\n",
    "            finder.apply_freq_filter(7)\n",
    "            return finder.nbest(bigram_measures.pmi, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "90872305-5f7a-4d08-98bc-acd0c8198395"
    }
   },
   "outputs": [],
   "source": [
    "def gatherCategorizedEvents():\n",
    "    allCategorizedEvents = []\n",
    "    allEvents = events_ml_collection.find({}, {\"category\": 1, \"description\": 1, \"name\": 1, \"_id\": 0})\n",
    "    count = 0\n",
    "    for e in allEvents:\n",
    "        count += 1\n",
    "        if 'category' in e and 'description' in e and 'name' in e:\n",
    "            allCategorizedEvents.append(e)\n",
    "    modernEvents = reduceCategories(allCategorizedEvents)\n",
    "    print count, \"total events, learning from the\", len(modernEvents), \"well categorized events\"\n",
    "    return modernEvents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def someCurrentCategories():\n",
    "    \"\"\"Looks at current events for the categories list, to be used if Facebook changes its events in the future\"\"\"\n",
    "    allCategorizedEvents = []\n",
    "    allEvents = ucla_events_collection.find({}, {\"category\": 1, \"description\": 1, \"name\": 1, \"_id\": 0})\n",
    "    for e in allEvents:\n",
    "        if 'category' in e and 'description' in e and 'name' in e:\n",
    "            allCategorizedEvents.append(e)\n",
    "    skTarget = [e['category'] for e in allCategorizedEvents]\n",
    "    count = sorted(list(set(skTarget)))\n",
    "    print(count)\n",
    "    \n",
    "curListOfCategories = [u'ART', u'CAUSE', u'COMEDY_PERFORMANCE', u'DANCE', u'DRINKS', u'FILM', u'FITNESS', u'FOOD',\n",
    "                       u'GAMES', u'GARDENING', u'HEALTH', u'LITERATURE', u'MEETUP', u'MUSIC', u'NETWORKING', u'PARTY',\n",
    "                       u'RELIGION', u'SHOPPING', u'SPORTS', u'THEATER', u'WELLNESS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduceCategories(events):\n",
    "    \"\"\"OTHER will be discarded from the training data\"\"\"\n",
    "    categoryMapping = {\n",
    "        u'BOOK': u'LITERATURE',\n",
    "        u'COMEDY': u'COMEDY_PERFORMANCE',\n",
    "        u'CLASS': u'OTHER',\n",
    "        u'DINING': u'FOOD',\n",
    "        u'FAMILY': u'OTHER',\n",
    "        u'FESTIVAL': u'PARTY',\n",
    "        u'FOOD_TASTING': u'FOOD',\n",
    "        u'FUNDRAISER': u'CAUSE',\n",
    "        u'LECTURE': u'OTHER',\n",
    "        u'MOVIE': u'FILM',\n",
    "        u'NEIGHBORHOOD': u'OTHER',\n",
    "        u'NIGHTLIFE': u'OTHER',\n",
    "        u'RELIGIOUS': u'RELIGION',\n",
    "        u'VOLUNTEERING': u'CAUSE',\n",
    "        u'WORKSHOP': u'OTHER'\n",
    "    }\n",
    "    \n",
    "    for e in events:\n",
    "        category = e['category']\n",
    "        if category in categoryMapping:\n",
    "            e['category'] = categoryMapping[category]\n",
    "    reducedEvents = [e for e in events if e['category'] != u'OTHER']\n",
    "    return reducedEvents\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "nbpresent": {
     "id": "fb083146-ae02-494f-9da7-8ae1a483ef92"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6936 total events, learning from the 2828 well categorized events\n"
     ]
    }
   ],
   "source": [
    "X = gatherCategorizedEvents()\n",
    "skText = [e['name']+' '+e['description'] for e in X]\n",
    "skTarget = [e['category'] for e in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/ml/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "def train(classifier, X, y, trails=25):\n",
    "    scores = np.zeros(trails)\n",
    "    for i in tqdm(range(0, trails)):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=i)\n",
    "\n",
    "        classifier.fit(X_train, y_train)\n",
    "        scores[i] = (classifier.score(X_test, y_test))\n",
    "    print \"Average Accuracy over %d trials: %s\" % (trails, np.mean(scores))\n",
    "    classifier.fit(X, y)\n",
    "    return classifier\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictList(vectorizer, classifier, x):\n",
    "    print(x)\n",
    "    x = vectorizer.transform(x)\n",
    "    y_pred = classifier.predict(x)\n",
    "    print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def giveProbPerCategory(vectorizer, classifier, x, threshold=.15):\n",
    "    print(x)\n",
    "    x = vectorizer.transform(x)\n",
    "    y_pred = classifier.predict_proba(x)\n",
    "\n",
    "    strongest_category = ''\n",
    "    highest_match = 0\n",
    "    above_threshold = []\n",
    "    for i in range(len(classifier.classes_)):\n",
    "        \n",
    "        if y_pred[0][i] > highest_match:\n",
    "            highest_match = y_pred[0][i]\n",
    "            strongest_category = classifier.classes_[i]\n",
    "            \n",
    "        if y_pred[0][i] > threshold:\n",
    "            above_threshold.append(classifier.classes_[i])\n",
    "    \n",
    "    if not above_threshold:\n",
    "        return [strongest_category]\n",
    "    else:\n",
    "        return above_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "nbpresent": {
     "id": "2ce54c9a-1a7a-4d84-9972-922c9aa4db15"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:13<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy over 25 trials: 0.5043847241867043\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    " \n",
    "trial1 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', MultinomialNB()),\n",
    "])\n",
    " \n",
    "nbModel = train(trial1, skText, skTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:11<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy over 25 trials: 0.7001047120418847\n"
     ]
    }
   ],
   "source": [
    "trial2 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', MultinomialNB(alpha=0.05)),\n",
    "])\n",
    "\n",
    "nbModel = train(trial2, skText, skTarget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.7022687609075043 without stoplist \n",
    "\n",
    "0.7209773123909248 with my stoplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 49.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy over 25 trials: 0.7225130890052357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "stoplist = 'for a of the and to in ucla . / '.split()\n",
    "stoplist = nltk.corpus.stopwords.words('english') + stoplist + string.punctuation.split()\n",
    "\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# tokenize and build vocab\n",
    "X = vectorizer.fit_transform(skText)\n",
    "\n",
    "# print(vectorizer.vocabulary_)\n",
    "# print(vectorizer.idf_)\n",
    "\n",
    "nbModel = MultinomialNB(alpha=0.05)\n",
    "nbModel = train(nbModel, X, skTarget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.7177661431064574 portersteemer with matchNotX\n",
    "\n",
    "0.7091099476439789 just snowball stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 45.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy over 25 trials: 0.7057425742574257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def matchNotX(strg, search=re.compile(r'[^!#$%&()*+,-./:;<=>?@\\\\^_}|{~0123456789]').search):\n",
    "    \"\"\"make sure word has something than punctuation\"\"\"\n",
    "    return bool(search(strg)) #make sure word has something other than punctuation\n",
    "\n",
    "def stemming_tokenizer(text):\n",
    "    stemmer = SnowballStemmer('english') #used to remove plural version of words\n",
    "    tokenizer = word_tokenize\n",
    "    return [stemmer.stem(w) for w in tokenizer(text)]\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=stemming_tokenizer, stop_words='english') \n",
    "X = vectorizer.fit_transform(skText)\n",
    "nbModel = MultinomialNB(alpha=0.05)\n",
    "nbModel = train(nbModel, X, skTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'A Funny Thing Happened on the Way to the Gynecologic Oncology A Funny Thing Happened on the Way to the Gynecologic Oncology Unit at Memorial Sloan Kettering Cancer Center of New York City\\nSeptember 5 - October 8, 2017 \\nGil Cates Theater\\nWritten by Halley Feiffer\\nDirected by Trip Cullman\\nFeaturing Halley Feiffer, Jason Butler Harner, Eileen T\\'Kaye & JoBeth Williams\\n\\n\"Under Trip Cullman\\'s perceptive direction, Halley Feiffer\\'s distinct voice is on fine display throughout, in all its uniquely unsettling glory.\"  \\u2013 TheaterMania \\n\\nSitting bedside at Memorial Sloan Kettering has never been so entertaining. While their ailing mothers share a hospital room, Karla and Don discover truth in the old clich\\xe9 that opposites attract\\u2026and repel\\u2026and attract.']\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-412ecbb34287>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgiveProbPerCategory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskText\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-c2fff0c068f9>\u001b[0m in \u001b[0;36mgiveProbPerCategory\u001b[0;34m(vectorizer, classifier, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me_pref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mcategory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "giveProbPerCategory(vectorizer, nbModel, skText[5:6])\n",
    "def giveProbPerCategory(vectorizer, classifier, x, threshold=.15):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04\n",
      "03\n",
      "05\n",
      "09\n",
      "11\n",
      "12\n",
      "19\n",
      "18\n",
      "66\n",
      "68\n",
      "89\n",
      "98\n",
      "91\n",
      "90\n",
      "94\n",
      "96\n",
      "02\n",
      "92\n"
     ]
    }
   ],
   "source": [
    "tokenizer = vectorizer.build_tokenizer()\n",
    "\n",
    "for w in vectorizer.vocabulary_:\n",
    "    if not matchNotX(w):\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'skTarget' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-c425fe5e3d58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskTarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'skTarget' is not defined"
     ]
    }
   ],
   "source": [
    "sorted(list(set(skTarget)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'ART', u'CAUSE', u'COMEDY_PERFORMANCE', u'CONFERENCE', u'DANCE', u'FILM', u'FOOD', u'GARDENING', u'HEALTH', u'LITERATURE', u'MUSIC', u'NEIGHBORHOOD', u'NETWORKING', u'PARTY', u'SPORTS', u'THEATER', u'WELLNESS']\n"
     ]
    }
   ],
   "source": [
    "someCurrentCategories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
