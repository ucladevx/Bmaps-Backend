{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "92f7983f-ec8a-41dd-987a-913b1754d3a4"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jfuentes/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /Users/jfuentes/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /Users/jfuentes/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/jfuentes/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jfuentes/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('perluniprops')\n",
    "nltk.download('nonbreaking_prefixes')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "e9a9474a-3d31-4794-be29-34bad530e8ea"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening the secret door...\n",
      "Got the .env secrets...\n",
      "Connected to database!\n",
      "Got database collections...\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /Users/jfuentes/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /Users/jfuentes/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jfuentes/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/jfuentes/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Login manager set up with auth Blueprint!\n"
     ]
    }
   ],
   "source": [
    "##Needed to get access to mappening.utils.database since this is under mappening.ml\n",
    "import sys\n",
    "sys.path.insert(0,'./../..')\n",
    "\n",
    "from mappening.utils.database import ucla_events_collection, events_ml_collection\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from nltk.tokenize.moses import MosesTokenizer\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from gensim.models import Phrases\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "b9cffde6-aea9-4d79-914e-6958234c679b"
    }
   },
   "outputs": [],
   "source": [
    "class PreprocessText:\n",
    "    def __init__(self, categorizedEvents):\n",
    "        \"\"\"categorizedEvents should be a list of dictionaries each corresponding to an event\n",
    "            X is the tokenized preprocessed text\n",
    "            Y is the corresponding categories\n",
    "            phraseMl is the phrase model that can further trained and used\n",
    "            phrases is a list of all the phrases identified\"\"\"\n",
    "        self.X = []\n",
    "        self.Y = []\n",
    "        tokenizer = MosesTokenizer() #tokenizers are basically an advanced split\n",
    "        for e in categorizedEvents:\n",
    "            text = e[u'name'] + \" \" + e[u'description']\n",
    "            text = tokenizer.tokenize(text)\n",
    "            text = self.preprocess(text)\n",
    "            self.X.append(text)\n",
    "            self.Y.append(e[u'category'])\n",
    "        self.phraseMl = Phrases(self.X, min_count=3) #learn ml model for phrase\n",
    "        self.X = list(self.phraseMl[self.X]) #use ml model for phrases\n",
    "#         self.X = list(self.phraseMl[self.X]) #get triples\n",
    "        self.phrases = phrases = set([w for doc in self.X for w in doc if '_' in w])\n",
    "        \n",
    "    def matchNotX(self, strg, search=re.compile(r'[^!#$%&()*+,-./:;<=>?@\\\\^_}|{~0123456789]').search):\n",
    "        \"\"\"make sure word has something than punctuation\"\"\"\n",
    "        return bool(search(strg)) #make sure word has something other than punctuation\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Remove all useless words and punct, make lowercase\"\"\"\n",
    "        stoplist = set('for a of the and to in . / '.split())\n",
    "        stoplist = set(nltk.corpus.stopwords.words('english')) | stoplist | set(string.punctuation)\n",
    "        return [word.strip(string.punctuation).lower() for word in text if word not in stoplist and self.matchNotX(word)]    \n",
    "        \n",
    "    def topBigrams(self, texts, n, tri=False):\n",
    "        \"\"\"Other method of getting phrases, currently unused because phrases can be further trained(online) and saved\"\"\"\n",
    "        flatTexts = []\n",
    "        for text in texts:\n",
    "            for word in text:\n",
    "                flatTexts.append(word)\n",
    "        bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "        trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "        topAnswers = []\n",
    "        if tri:\n",
    "            finder = nltk.collocations.TrigramCollocationFinder.from_words(flatTexts)\n",
    "            finder.apply_freq_filter(7)\n",
    "            return finder.nbest(trigram_measures.pmi, n)\n",
    "        else:\n",
    "            finder = nltk.collocations.BigramCollocationFinder.from_words(flatTexts)\n",
    "            finder.apply_freq_filter(7)\n",
    "            return finder.nbest(bigram_measures.pmi, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "90872305-5f7a-4d08-98bc-acd0c8198395"
    }
   },
   "outputs": [],
   "source": [
    "def gatherCategorizedEvents():\n",
    "    \"\"\"Return panda dataframe of events with category, description, and name\"\"\"\n",
    "    allCategorizedEvents = []\n",
    "    allEvents = events_ml_collection.find({}, {\"category\": 1, \"description\": 1, \"name\": 1, \"hoster\": 1, \"_id\": 0})\n",
    "    count = 0\n",
    "    for e in allEvents:\n",
    "        count += 1\n",
    "        e['hoster'] = e['hoster']['name']\n",
    "        if 'category' in e and 'description' in e and 'name' in e:\n",
    "            allCategorizedEvents.append(e)\n",
    "    modernEvents = reduceCategories(allCategorizedEvents)\n",
    "    print count, \"total events, learning from the\", len(modernEvents), \"well categorized events\"\n",
    "    return pd.DataFrame(modernEvents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def someCurrentCategories():\n",
    "    \"\"\"Looks at current events for the categories list, to be used if Facebook changes its events in the future\"\"\"\n",
    "    allCategorizedEvents = []\n",
    "    allEvents = ucla_events_collection.find({}, {\"category\": 1, \"description\": 1, \"name\": 1, \"_id\": 0})\n",
    "    for e in allEvents:\n",
    "        if 'category' in e and 'description' in e and 'name' in e:\n",
    "            allCategorizedEvents.append(e)\n",
    "    skTarget = [e['category'] for e in allCategorizedEvents]\n",
    "    count = sorted(list(set(skTarget)))\n",
    "    print(count)\n",
    "    \n",
    "curListOfCategories = [u'ART', u'CAUSE', u'COMEDY_PERFORMANCE', u'DANCE', u'DRINKS', u'FILM', u'FITNESS', u'FOOD',\n",
    "                       u'GAMES', u'GARDENING', u'HEALTH', u'LITERATURE', u'MEETUP', u'MUSIC', u'NETWORKING', u'PARTY',\n",
    "                       u'RELIGION', u'SHOPPING', u'SPORTS', u'THEATER', u'WELLNESS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduceCategories(events):\n",
    "    \"\"\"OTHER will be discarded from the training data\"\"\"\n",
    "    categoryMapping = {\n",
    "        u'BOOK': u'LITERATURE',\n",
    "        u'COMEDY': u'COMEDY_PERFORMANCE',\n",
    "        u'CLASS': u'OTHER',\n",
    "        u'DINING': u'FOOD',\n",
    "        u'FAMILY': u'OTHER',\n",
    "        u'FESTIVAL': u'PARTY',\n",
    "        u'FOOD_TASTING': u'FOOD',\n",
    "        u'FUNDRAISER': u'CAUSE',\n",
    "        u'LECTURE': u'OTHER',\n",
    "        u'MOVIE': u'FILM',\n",
    "        u'NEIGHBORHOOD': u'OTHER',\n",
    "        u'NIGHTLIFE': u'OTHER',\n",
    "        u'RELIGIOUS': u'RELIGION',\n",
    "        u'VOLUNTEERING': u'CAUSE',\n",
    "        u'WORKSHOP': u'OTHER'\n",
    "    }\n",
    "    \n",
    "    for e in events:\n",
    "        category = e['category']\n",
    "        if category in categoryMapping:\n",
    "            e['category'] = categoryMapping[category]\n",
    "    reducedEvents = [e for e in events if e['category'] != u'OTHER']\n",
    "    return reducedEvents\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbpresent": {
     "id": "fb083146-ae02-494f-9da7-8ae1a483ef92"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6936 total events, learning from the 2828 well categorized events\n"
     ]
    }
   ],
   "source": [
    "X = gatherCategorizedEvents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/ml/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "def train(classifier, X, y, trails=25):\n",
    "    scores = np.zeros(trails)\n",
    "    for i in tqdm(range(0, trails)):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=i)\n",
    "\n",
    "        classifier.fit(X_train, y_train)\n",
    "        scores[i] = (classifier.score(X_test, y_test))\n",
    "    print \"Average Accuracy over %d trials: %s\" % (trails, np.mean(scores))\n",
    "    classifier.fit(X, y)\n",
    "    return classifier\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictList(vectorizer, classifier, x):\n",
    "    print(x)\n",
    "    x = vectorizer.transform(x)\n",
    "    y_pred = classifier.predict(x)\n",
    "    print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictCategoryProbability(nameVectorizer, detailVectorizer, classifier, X, threshold=.1):\n",
    "    X_name_transform = nameVectorizer.transform(X['name'])\n",
    "    X_details_transform = detailVectorizer.transform(X['description'])\n",
    "    X_total_transform = hstack([X_name_transform, X_details_transform])\n",
    "    y_pred = classifier.predict_proba(X_total_transform)\n",
    "    y_categories = []\n",
    "    \n",
    "    for i in range(0, X_total_transform.shape[0]):\n",
    "        current_categories_probabilities = [] #tuple of category name and prob\n",
    "        \n",
    "        #create tuple of category, class\n",
    "        for j in range(0, len(classifier.classes_)):\n",
    "            current_categories_probabilities.append((classifier.classes_[j], y_pred[i][j]))\n",
    "        \n",
    "        current_categories_probabilities.sort(key=lambda x: x[1], reverse=True) #put highest prob categories first\n",
    "        current_categories = []\n",
    "        for k, cp in enumerate(current_categories_probabilities):\n",
    "            if k == 0 or cp[1] > threshold: #ensures at least one cat \n",
    "                current_categories.append(cp[0])\n",
    "        y_categories.append(current_categories)\n",
    "        \n",
    "    return y_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbpresent": {
     "id": "2ce54c9a-1a7a-4d84-9972-922c9aa4db15"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 69.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy over 25 trials: 0.5812164073550212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    " \n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# tokenize and build vocab\n",
    "X_name_transform = vectorizer.fit_transform(X['name'])\n",
    "X_details_transform = vectorizer.fit_transform(X['description'])\n",
    "# print(vectorizer.vocabulary_)\n",
    "# print(vectorizer.idf_)\n",
    "\n",
    "nbModel = MultinomialNB()\n",
    "nbModel = train(nbModel, X_name_transform, X['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMBINE NAME AND DESCRIPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:01<00:00, 22.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy over 25 trials: 0.5854596888260254\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    " \n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# tokenize and build vocab\n",
    "X_name_transform = vectorizer.fit_transform(X['name'])\n",
    "X_details_transform = vectorizer.fit_transform(X['description'])\n",
    "X_total_transform = hstack([X_name_transform, X_details_transform])\n",
    "\n",
    "nbModel = MultinomialNB()\n",
    "nbModel = train(nbModel, X_total_transform, X['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# tokenize and build vocab\n",
    "X_name_transform = vectorizer.fit_transform(X['name'])\n",
    "X_details_transform = vectorizer.fit_transform(X['description'])\n",
    "X_total_transform = hstack([X_name_transform, X_details_transform])\n",
    "\n",
    "nbModel = MultinomialNB()\n",
    "param = {'alpha': [.01, .04, .05, .06, .3, .9, 1.5]}\n",
    "gs = GridSearchCV(nbModel, param)\n",
    "gs.fit(X_total_transform, X['category'])\n",
    "pd.DataFrame(gs.cv_results_).sort_values('mean_test_score', ascending=False)[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STOPLIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# tokenize and build vocab\n",
    "X_name_transform = vectorizer.fit_transform(X['name'])\n",
    "X_details_transform = vectorizer.fit_transform(X['description'])\n",
    "X_total_transform = hstack([X_name_transform, X_details_transform])\n",
    "\n",
    "nbModel = MultinomialNB()\n",
    "param = {'alpha': [.01, .04, .05, .06, .3, .9, 1.5]}\n",
    "gs = GridSearchCV(nbModel, param)\n",
    "gs.fit(X_total_transform, X['category'])\n",
    "pd.DataFrame(gs.cv_results_).sort_values('mean_test_score', ascending=False)[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from nltk import WordNetLemmatizer, word_tokenize\n",
    "\n",
    "def lemma_tokenizer(text):\n",
    "    wn = WordNetLemmatizer()\n",
    "    tokenizer = word_tokenize\n",
    "    return [wn.lemmatize(w) for w in tokenizer(text)]\n",
    "\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer(stop_words='english', analyzer=lemma_tokenizer)\n",
    "\n",
    "# tokenize and build vocab\n",
    "X_name_transform = vectorizer.fit_transform(X['name'])\n",
    "X_details_transform = vectorizer.fit_transform(X['description'])\n",
    "X_total_transform = hstack([X_name_transform, X_details_transform])\n",
    "\n",
    "nbModel = MultinomialNB()\n",
    "param = {'alpha': [.01, .04, .05, .06, .3, .9, 1.5]}\n",
    "gs = GridSearchCV(nbModel, param)\n",
    "gs.fit(X_total_transform, X['category'])\n",
    "pd.DataFrame(gs.cv_results_).sort_values('mean_test_score', ascending=False)[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NGrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "for ngrams in [(1,1), (1,2), (1,3), (2,2), (2,3), (3,3)]:\n",
    "    # create the transform\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', ngram_range=ngrams)\n",
    "\n",
    "    # tokenize and build vocab\n",
    "    X_name_transform = vectorizer.fit_transform(X['name'])\n",
    "    X_details_transform = vectorizer.fit_transform(X['description'])\n",
    "    X_total_transform = hstack([X_name_transform, X_details_transform])\n",
    "\n",
    "    nbModel = MultinomialNB(alpha=.04)\n",
    "    print(ngrams)\n",
    "    nbModel = train(nbModel, X_total_transform, X['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# tokenize and build vocab\n",
    "X_name_transform = vectorizer.fit_transform(X['name'])\n",
    "X_details_transform = vectorizer.fit_transform(X['description'])\n",
    "X_total_transform = hstack([X_name_transform, X_details_transform])\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "param = {'n_estimators': [10, 150, 300],\n",
    "          'max_depth': [30,60,90, None]}\n",
    "gs = GridSearchCV(rf, param, cv=5, n_jobs=-1)\n",
    "gs.fit(X_total_transform, X['category'])\n",
    "pd.DataFrame(gs.cv_results_).sort_values('mean_test_score', ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Booster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# tokenize and build vocab\n",
    "X_name_transform = vectorizer.fit_transform(X['name'])\n",
    "X_details_transform = vectorizer.fit_transform(X['description'])\n",
    "X_total_transform = hstack([X_name_transform, X_details_transform])\n",
    "\n",
    "gb = GradientBoostingClassifier()\n",
    "param = {'n_estimators': [10, 150, 300]}\n",
    "gs = GridSearchCV(gb, param, cv=5, n_jobs=-1)\n",
    "gs.fit(X_total_transform, X['category'])\n",
    "pd.DataFrame(gs.cv_results_).sort_values('mean_test_score', ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# FINAL PRODUCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy over 1 trials: 0.7015558698727016\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# create the transform\n",
    "nameVectorizer = TfidfVectorizer(stop_words='english')\n",
    "detailVectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# tokenize and build vocab\n",
    "X_name_transform = nameVectorizer.fit_transform(X['name'])\n",
    "X_details_transform = detailVectorizer.fit_transform(X['description'])\n",
    "X_total_transform = hstack([X_name_transform, X_details_transform])\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=150, max_depth=None)\n",
    "rf = train(rf, X_total_transform, X['category'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cPickle\n",
    "\n",
    "with open(r\"categorizationModel.pickle\", \"wb\") as output_file:\n",
    "    cPickle.dump(rf, output_file)\n",
    "    \n",
    "with open(r\"nameVectorizer.pickle\", \"wb\") as output_file:\n",
    "    cPickle.dump(nameVectorizer, output_file)\n",
    "    \n",
    "with open(r\"detailVectorizer.pickle\", \"wb\") as output_file:\n",
    "    cPickle.dump(detailVectorizer, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(r\"categorizationModel.pickle\", \"r\") as model:\n",
    "    rfLoaded = cPickle.load(model)\n",
    "    \n",
    "with open(r\"nameVectorizer.pickle\", \"r\") as model:\n",
    "    nameVLoaded = cPickle.load(model)\n",
    "    \n",
    "with open(r\"detailVectorizer.pickle\", \"r\") as model:\n",
    "    detailVLoaded = cPickle.load(model)\n",
    "\n",
    "X_name_transform = nameVLoaded.transform(X['name'])\n",
    "X_details_transform = detailVLoaded.transform(X['description'])\n",
    "X_total_transform = hstack([X_name_transform, X_details_transform])\n",
    "\n",
    "x = predictCategoryProbability(nameVectorizer, detailVectorizer, rfLoaded, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'COMEDY_PERFORMANCE', u'MUSIC'],\n",
       " [u'FILM'],\n",
       " [u'THEATER'],\n",
       " [u'NETWORKING'],\n",
       " [u'THEATER'],\n",
       " [u'FOOD'],\n",
       " [u'LITERATURE'],\n",
       " [u'THEATER'],\n",
       " [u'FOOD'],\n",
       " [u'THEATER', u'FILM'],\n",
       " [u'THEATER'],\n",
       " [u'ART', u'MUSIC'],\n",
       " [u'THEATER'],\n",
       " [u'MEETUP'],\n",
       " [u'MUSIC'],\n",
       " [u'CAUSE'],\n",
       " [u'FOOD'],\n",
       " [u'CAUSE'],\n",
       " [u'MUSIC'],\n",
       " [u'NETWORKING'],\n",
       " [u'COMEDY_PERFORMANCE'],\n",
       " [u'FOOD', u'CAUSE'],\n",
       " [u'PARTY', u'FOOD'],\n",
       " [u'FOOD'],\n",
       " [u'SPORTS'],\n",
       " [u'FOOD'],\n",
       " [u'CAUSE'],\n",
       " [u'PARTY'],\n",
       " [u'CAUSE'],\n",
       " [u'MUSIC'],\n",
       " [u'CAUSE'],\n",
       " [u'ART'],\n",
       " [u'WELLNESS'],\n",
       " [u'MEETUP'],\n",
       " [u'FOOD'],\n",
       " [u'MUSIC'],\n",
       " [u'SPORTS'],\n",
       " [u'HEALTH'],\n",
       " [u'PARTY'],\n",
       " [u'FOOD'],\n",
       " [u'CAUSE'],\n",
       " [u'CAUSE'],\n",
       " [u'CAUSE'],\n",
       " [u'THEATER'],\n",
       " [u'CAUSE', u'FOOD'],\n",
       " [u'SPORTS'],\n",
       " [u'ART'],\n",
       " [u'THEATER'],\n",
       " [u'CAUSE', u'FOOD'],\n",
       " [u'MUSIC'],\n",
       " [u'FOOD'],\n",
       " [u'MEETUP'],\n",
       " [u'SPORTS'],\n",
       " [u'MEETUP', u'FILM'],\n",
       " [u'CAUSE'],\n",
       " [u'SPORTS'],\n",
       " [u'MEETUP'],\n",
       " [u'MEETUP'],\n",
       " [u'THEATER'],\n",
       " [u'SPORTS'],\n",
       " [u'SPORTS'],\n",
       " [u'THEATER', u'FILM'],\n",
       " [u'THEATER', u'CAUSE'],\n",
       " [u'CAUSE'],\n",
       " [u'FILM'],\n",
       " [u'CAUSE'],\n",
       " [u'THEATER', u'MUSIC'],\n",
       " [u'CAUSE'],\n",
       " [u'FOOD'],\n",
       " [u'MUSIC'],\n",
       " [u'MUSIC'],\n",
       " [u'MEETUP'],\n",
       " [u'PARTY'],\n",
       " [u'THEATER', u'MUSIC'],\n",
       " [u'FILM'],\n",
       " [u'NETWORKING'],\n",
       " [u'CAUSE'],\n",
       " [u'MUSIC'],\n",
       " [u'COMEDY_PERFORMANCE'],\n",
       " [u'THEATER'],\n",
       " [u'NETWORKING'],\n",
       " [u'CAUSE'],\n",
       " [u'CAUSE'],\n",
       " [u'THEATER'],\n",
       " [u'RELIGION'],\n",
       " [u'NETWORKING'],\n",
       " [u'CAUSE'],\n",
       " [u'SPORTS'],\n",
       " [u'SPORTS'],\n",
       " [u'MEETUP', u'FOOD'],\n",
       " [u'CAUSE'],\n",
       " [u'NETWORKING', u'CAUSE'],\n",
       " [u'COMEDY_PERFORMANCE', u'FILM'],\n",
       " [u'RELIGION', u'MEETUP'],\n",
       " [u'CAUSE'],\n",
       " [u'ART'],\n",
       " [u'THEATER'],\n",
       " [u'FILM'],\n",
       " [u'SPORTS'],\n",
       " [u'FOOD', u'CAUSE'],\n",
       " [u'GAMES', u'FOOD'],\n",
       " [u'SPORTS'],\n",
       " [u'NETWORKING'],\n",
       " [u'ART', u'THEATER', u'CAUSE'],\n",
       " [u'SPORTS'],\n",
       " [u'FOOD'],\n",
       " [u'FOOD'],\n",
       " [u'THEATER', u'FILM'],\n",
       " [u'CONFERENCE'],\n",
       " [u'MUSIC'],\n",
       " [u'FOOD'],\n",
       " [u'FITNESS'],\n",
       " [u'CAUSE'],\n",
       " [u'NETWORKING', u'CAUSE'],\n",
       " [u'PARTY', u'FOOD'],\n",
       " [u'CONFERENCE'],\n",
       " [u'SPORTS', u'CAUSE'],\n",
       " [u'ART'],\n",
       " [u'SPORTS'],\n",
       " [u'PARTY', u'MUSIC'],\n",
       " [u'DANCE'],\n",
       " [u'THEATER'],\n",
       " [u'ART', u'THEATER'],\n",
       " [u'CAUSE'],\n",
       " [u'THEATER', u'FILM'],\n",
       " [u'DANCE'],\n",
       " [u'THEATER'],\n",
       " [u'NETWORKING'],\n",
       " [u'FILM'],\n",
       " [u'NETWORKING'],\n",
       " [u'SHOPPING'],\n",
       " [u'THEATER'],\n",
       " [u'THEATER'],\n",
       " [u'CAUSE'],\n",
       " [u'ART', u'NETWORKING'],\n",
       " [u'FOOD'],\n",
       " [u'FILM'],\n",
       " [u'CONFERENCE'],\n",
       " [u'FOOD'],\n",
       " [u'NETWORKING'],\n",
       " [u'NETWORKING', u'FOOD'],\n",
       " [u'COMEDY_PERFORMANCE'],\n",
       " [u'FILM'],\n",
       " [u'PARTY', u'FOOD'],\n",
       " [u'FOOD'],\n",
       " [u'HEALTH', u'CAUSE'],\n",
       " [u'THEATER'],\n",
       " [u'CAUSE', u'MUSIC'],\n",
       " [u'THEATER'],\n",
       " [u'MEETUP'],\n",
       " [u'MUSIC', u'FILM'],\n",
       " [u'MEETUP'],\n",
       " [u'FILM'],\n",
       " [u'MEETUP'],\n",
       " [u'WELLNESS'],\n",
       " [u'ART'],\n",
       " [u'CAUSE', u'MUSIC'],\n",
       " [u'PARTY'],\n",
       " [u'CAUSE'],\n",
       " [u'CAUSE'],\n",
       " [u'SPORTS'],\n",
       " [u'MEETUP'],\n",
       " [u'MUSIC'],\n",
       " [u'CAUSE'],\n",
       " [u'CAUSE', u'FOOD'],\n",
       " [u'CAUSE'],\n",
       " [u'SPORTS'],\n",
       " [u'MUSIC'],\n",
       " [u'SPORTS'],\n",
       " [u'FILM'],\n",
       " [u'FOOD'],\n",
       " [u'FOOD'],\n",
       " [u'THEATER'],\n",
       " [u'MEETUP'],\n",
       " [u'CAUSE'],\n",
       " [u'CAUSE'],\n",
       " [u'COMEDY_PERFORMANCE'],\n",
       " [u'FITNESS', u'CAUSE'],\n",
       " [u'CAUSE'],\n",
       " [u'ART', u'MUSIC'],\n",
       " [u'THEATER', u'FILM'],\n",
       " [u'THEATER'],\n",
       " [u'THEATER'],\n",
       " [u'THEATER', u'FILM'],\n",
       " [u'CAUSE'],\n",
       " [u'THEATER'],\n",
       " [u'SPORTS'],\n",
       " [u'ART'],\n",
       " [u'SPORTS'],\n",
       " [u'FOOD', u'NETWORKING'],\n",
       " [u'MEETUP'],\n",
       " [u'SPORTS'],\n",
       " [u'MEETUP'],\n",
       " [u'FILM', u'THEATER'],\n",
       " [u'NETWORKING'],\n",
       " [u'SPORTS'],\n",
       " [u'SPORTS'],\n",
       " [u'MEETUP'],\n",
       " [u'HEALTH'],\n",
       " [u'FILM'],\n",
       " [u'COMEDY_PERFORMANCE'],\n",
       " [u'FITNESS'],\n",
       " [u'CAUSE'],\n",
       " [u'CAUSE'],\n",
       " [u'ART'],\n",
       " [u'SPORTS'],\n",
       " [u'DANCE'],\n",
       " [u'NETWORKING'],\n",
       " [u'MEETUP', u'FOOD'],\n",
       " [u'FILM'],\n",
       " [u'FOOD'],\n",
       " [u'FOOD'],\n",
       " [u'NETWORKING'],\n",
       " [u'THEATER'],\n",
       " [u'FILM'],\n",
       " [u'CONFERENCE'],\n",
       " [u'CAUSE'],\n",
       " [u'ART'],\n",
       " [u'FOOD', u'CAUSE'],\n",
       " [u'FILM'],\n",
       " [u'CAUSE'],\n",
       " [u'MEETUP'],\n",
       " [u'THEATER', u'FILM'],\n",
       " [u'SPORTS'],\n",
       " [u'THEATER'],\n",
       " [u'CAUSE'],\n",
       " [u'CAUSE'],\n",
       " [u'MUSIC'],\n",
       " [u'MUSIC'],\n",
       " [u'PARTY', u'CAUSE'],\n",
       " [u'CAUSE'],\n",
       " [u'THEATER'],\n",
       " [u'COMEDY_PERFORMANCE'],\n",
       " [u'THEATER', u'FILM'],\n",
       " [u'SPORTS', u'MEETUP'],\n",
       " [u'FOOD'],\n",
       " [u'CAUSE'],\n",
       " [u'MUSIC'],\n",
       " [u'FOOD'],\n",
       " [u'RELIGION'],\n",
       " [u'DANCE'],\n",
       " [u'CAUSE'],\n",
       " [u'THEATER'],\n",
       " [u'MUSIC'],\n",
       " [u'PARTY'],\n",
       " [u'SPORTS'],\n",
       " [u'CAUSE'],\n",
       " [u'NETWORKING'],\n",
       " [u'SPORTS'],\n",
       " [u'THEATER'],\n",
       " [u'FOOD', u'CAUSE'],\n",
       " [u'COMEDY_PERFORMANCE'],\n",
       " [u'MUSIC'],\n",
       " [u'FILM'],\n",
       " [u'THEATER'],\n",
       " [u'THEATER', u'COMEDY_PERFORMANCE'],\n",
       " [u'SPORTS'],\n",
       " [u'SPORTS'],\n",
       " [u'FILM', u'ART'],\n",
       " [u'THEATER'],\n",
       " [u'MUSIC', u'THEATER'],\n",
       " [u'ART'],\n",
       " [u'COMEDY_PERFORMANCE'],\n",
       " [u'CAUSE', u'FOOD'],\n",
       " [u'GAMES'],\n",
       " [u'MUSIC'],\n",
       " [u'RELIGION'],\n",
       " [u'THEATER'],\n",
       " [u'SPORTS'],\n",
       " [u'CAUSE'],\n",
       " [u'MEETUP', u'NETWORKING'],\n",
       " [u'FOOD'],\n",
       " [u'NETWORKING'],\n",
       " [u'HEALTH'],\n",
       " [u'THEATER', u'FILM'],\n",
       " [u'FITNESS', u'FOOD'],\n",
       " [u'FOOD', u'CAUSE'],\n",
       " [u'SPORTS'],\n",
       " [u'CAUSE'],\n",
       " [u'SPORTS'],\n",
       " [u'SPORTS'],\n",
       " [u'SPORTS'],\n",
       " [u'FILM'],\n",
       " [u'NETWORKING'],\n",
       " [u'THEATER'],\n",
       " [u'ART'],\n",
       " [u'NETWORKING'],\n",
       " [u'CAUSE'],\n",
       " [u'MUSIC'],\n",
       " [u'ART'],\n",
       " [u'SPORTS'],\n",
       " [u'SPORTS'],\n",
       " [u'CAUSE'],\n",
       " [u'MEETUP', u'CAUSE', u'FOOD'],\n",
       " [u'SPORTS'],\n",
       " [u'ART', u'MUSIC'],\n",
       " [u'CAUSE'],\n",
       " [u'MEETUP'],\n",
       " [u'THEATER'],\n",
       " [u'SPORTS'],\n",
       " [u'PARTY'],\n",
       " [u'WELLNESS', u'FOOD'],\n",
       " [u'CAUSE'],\n",
       " [u'THEATER'],\n",
       " [u'COMEDY_PERFORMANCE'],\n",
       " [u'CAUSE'],\n",
       " [u'MEETUP', u'CAUSE'],\n",
       " [u'FOOD'],\n",
       " [u'RELIGION', u'CAUSE'],\n",
       " [u'MUSIC'],\n",
       " [u'THEATER'],\n",
       " [u'SPORTS'],\n",
       " [u'FILM'],\n",
       " [u'CAUSE', u'FOOD'],\n",
       " [u'MEETUP'],\n",
       " [u'MEETUP'],\n",
       " [u'ART'],\n",
       " [u'NETWORKING'],\n",
       " [u'CAUSE'],\n",
       " [u'PARTY'],\n",
       " [u'CAUSE', u'FOOD'],\n",
       " [u'NETWORKING'],\n",
       " [u'ART', u'MUSIC'],\n",
       " [u'ART', u'MUSIC'],\n",
       " [u'ART'],\n",
       " [u'CONFERENCE', u'CAUSE'],\n",
       " [u'MUSIC'],\n",
       " [u'COMEDY_PERFORMANCE'],\n",
       " [u'FOOD'],\n",
       " [u'SPORTS'],\n",
       " [u'SPORTS'],\n",
       " [u'CAUSE'],\n",
       " [u'ART'],\n",
       " [u'NETWORKING'],\n",
       " [u'CRAFTS'],\n",
       " [u'CAUSE'],\n",
       " [u'THEATER'],\n",
       " [u'THEATER'],\n",
       " [u'WELLNESS', u'CAUSE'],\n",
       " [u'FOOD', u'CAUSE'],\n",
       " [u'MEETUP', u'CAUSE'],\n",
       " [u'ART', u'CAUSE'],\n",
       " [u'NETWORKING'],\n",
       " [u'CAUSE'],\n",
       " [u'MUSIC'],\n",
       " [u'FILM', u'SPORTS'],\n",
       " [u'THEATER', u'FILM'],\n",
       " [u'SPORTS'],\n",
       " [u'ART'],\n",
       " [u'CONFERENCE', u'CAUSE'],\n",
       " [u'FILM'],\n",
       " [u'FILM'],\n",
       " [u'FOOD', u'CAUSE'],\n",
       " [u'CAUSE'],\n",
       " [u'FOOD'],\n",
       " [u'ART'],\n",
       " [u'MUSIC'],\n",
       " [u'CAUSE'],\n",
       " [u'CAUSE'],\n",
       " [u'MUSIC'],\n",
       " [u'DANCE'],\n",
       " [u'CAUSE'],\n",
       " [u'THEATER'],\n",
       " [u'PARTY'],\n",
       " [u'ART'],\n",
       " [u'THEATER'],\n",
       " [u'THEATER', u'COMEDY_PERFORMANCE', u'MUSIC'],\n",
       " [u'ART'],\n",
       " [u'COMEDY_PERFORMANCE'],\n",
       " [u'ART', u'CAUSE'],\n",
       " [u'CAUSE'],\n",
       " [u'HEALTH'],\n",
       " [u'CAUSE'],\n",
       " [u'PARTY'],\n",
       " [u'FITNESS'],\n",
       " [u'FILM'],\n",
       " [u'FILM'],\n",
       " [u'CAUSE'],\n",
       " [u'THEATER', u'MUSIC'],\n",
       " [u'FOOD'],\n",
       " [u'NETWORKING'],\n",
       " [u'CAUSE', u'FILM'],\n",
       " [u'SPORTS'],\n",
       " [u'FOOD'],\n",
       " [u'FOOD'],\n",
       " [u'PARTY'],\n",
       " [u'PARTY'],\n",
       " [u'CAUSE'],\n",
       " [u'CAUSE'],\n",
       " [u'FILM'],\n",
       " [u'FILM'],\n",
       " [u'CAUSE'],\n",
       " [u'WELLNESS'],\n",
       " [u'MUSIC'],\n",
       " [u'ART'],\n",
       " [u'MUSIC'],\n",
       " [u'CONFERENCE', u'MEETUP'],\n",
       " [u'THEATER'],\n",
       " [u'NETWORKING'],\n",
       " [u'FOOD'],\n",
       " [u'THEATER'],\n",
       " [u'THEATER'],\n",
       " [u'CRAFTS', u'ART'],\n",
       " [u'GAMES', u'PARTY'],\n",
       " [u'GAMES'],\n",
       " [u'MUSIC', u'CAUSE'],\n",
       " [u'ART'],\n",
       " [u'THEATER'],\n",
       " [u'WELLNESS'],\n",
       " [u'CAUSE'],\n",
       " [u'ART'],\n",
       " [u'PARTY'],\n",
       " [u'FILM'],\n",
       " [u'NETWORKING'],\n",
       " [u'ART', u'THEATER'],\n",
       " [u'CAUSE'],\n",
       " [u'COMEDY_PERFORMANCE'],\n",
       " [u'FILM'],\n",
       " [u'FILM'],\n",
       " [u'SPORTS'],\n",
       " [u'NETWORKING', u'CAUSE'],\n",
       " [u'DRINK', u'CAUSE'],\n",
       " [u'PARTY', u'GAMES'],\n",
       " [u'WELLNESS', u'CAUSE'],\n",
       " [u'MUSIC'],\n",
       " [u'CAUSE'],\n",
       " [u'SPORTS'],\n",
       " [u'FILM'],\n",
       " [u'FOOD', u'MUSIC'],\n",
       " [u'NETWORKING', u'CAUSE'],\n",
       " [u'SPORTS'],\n",
       " [u'SPORTS'],\n",
       " [u'SPORTS'],\n",
       " [u'FILM'],\n",
       " [u'FOOD'],\n",
       " [u'FILM'],\n",
       " [u'ART'],\n",
       " [u'SPORTS'],\n",
       " [u'ART'],\n",
       " [u'CRAFTS', u'ART'],\n",
       " [u'CAUSE', u'MEETUP'],\n",
       " [u'ART'],\n",
       " [u'CAUSE'],\n",
       " [u'FOOD', u'CAUSE'],\n",
       " [u'SPORTS'],\n",
       " [u'FILM'],\n",
       " [u'SPORTS'],\n",
       " [u'ART'],\n",
       " [u'SPORTS'],\n",
       " [u'THEATER'],\n",
       " [u'FOOD'],\n",
       " [u'FILM'],\n",
       " [u'FILM'],\n",
       " [u'CAUSE'],\n",
       " [u'FOOD'],\n",
       " [u'SPORTS'],\n",
       " [u'NETWORKING'],\n",
       " [u'NETWORKING', u'FOOD'],\n",
       " [u'DRINK'],\n",
       " [u'THEATER'],\n",
       " [u'MUSIC'],\n",
       " [u'CAUSE'],\n",
       " [u'PARTY', u'FOOD'],\n",
       " [u'LITERATURE'],\n",
       " [u'CAUSE'],\n",
       " [u'CAUSE'],\n",
       " [u'THEATER'],\n",
       " [u'MUSIC'],\n",
       " [u'FITNESS', u'CAUSE'],\n",
       " [u'ART', u'CAUSE'],\n",
       " [u'THEATER'],\n",
       " [u'CAUSE'],\n",
       " [u'MUSIC'],\n",
       " [u'MEETUP'],\n",
       " [u'PARTY'],\n",
       " [u'COMEDY_PERFORMANCE', u'FOOD'],\n",
       " [u'FITNESS'],\n",
       " [u'CAUSE'],\n",
       " [u'MUSIC'],\n",
       " [u'FILM'],\n",
       " [u'CAUSE'],\n",
       " [u'CAUSE'],\n",
       " [u'PARTY'],\n",
       " [u'THEATER'],\n",
       " [u'THEATER'],\n",
       " [u'DANCE'],\n",
       " [u'MEETUP'],\n",
       " [u'CAUSE'],\n",
       " [u'CAUSE'],\n",
       " [u'THEATER', u'FILM'],\n",
       " [u'THEATER'],\n",
       " [u'PARTY'],\n",
       " [u'FOOD'],\n",
       " [u'MEETUP', u'CAUSE'],\n",
       " [u'THEATER'],\n",
       " [u'FILM'],\n",
       " [u'THEATER'],\n",
       " [u'CAUSE'],\n",
       " [u'NETWORKING'],\n",
       " [u'CAUSE'],\n",
       " [u'NETWORKING'],\n",
       " [u'CAUSE'],\n",
       " [u'THEATER'],\n",
       " [u'SPORTS'],\n",
       " [u'MUSIC'],\n",
       " [u'FOOD'],\n",
       " [u'FILM'],\n",
       " [u'GAMES'],\n",
       " [u'FILM'],\n",
       " [u'MEETUP'],\n",
       " [u'DRINK', u'FOOD'],\n",
       " [u'THEATER'],\n",
       " [u'SPORTS'],\n",
       " [u'FOOD', u'CAUSE'],\n",
       " [u'PARTY'],\n",
       " [u'THEATER'],\n",
       " [u'THEATER'],\n",
       " [u'THEATER'],\n",
       " [u'FOOD', u'MUSIC'],\n",
       " [u'FOOD', u'CAUSE'],\n",
       " [u'FILM', u'THEATER'],\n",
       " [u'THEATER'],\n",
       " [u'NETWORKING'],\n",
       " [u'MUSIC'],\n",
       " [u'THEATER'],\n",
       " [u'ART'],\n",
       " [u'FILM'],\n",
       " [u'CAUSE'],\n",
       " [u'FILM'],\n",
       " [u'FOOD'],\n",
       " [u'ART', u'CAUSE'],\n",
       " [u'SPORTS'],\n",
       " [u'SPORTS'],\n",
       " [u'DANCE'],\n",
       " [u'FILM'],\n",
       " [u'DANCE'],\n",
       " [u'CAUSE'],\n",
       " [u'CAUSE', u'FOOD'],\n",
       " [u'CAUSE'],\n",
       " [u'NETWORKING'],\n",
       " [u'SPORTS'],\n",
       " [u'ART'],\n",
       " [u'FILM'],\n",
       " [u'MUSIC', u'ART'],\n",
       " [u'CAUSE'],\n",
       " [u'FOOD', u'CAUSE'],\n",
       " [u'PARTY', u'FILM'],\n",
       " [u'THEATER', u'FILM'],\n",
       " [u'MUSIC', u'CAUSE'],\n",
       " [u'THEATER', u'COMEDY_PERFORMANCE'],\n",
       " [u'THEATER', u'FILM'],\n",
       " [u'THEATER'],\n",
       " [u'THEATER'],\n",
       " [u'MEETUP'],\n",
       " [u'PARTY', u'FOOD'],\n",
       " [u'MUSIC'],\n",
       " [u'FILM'],\n",
       " [u'FOOD'],\n",
       " [u'MEETUP'],\n",
       " [u'NETWORKING'],\n",
       " [u'MUSIC'],\n",
       " [u'MEETUP'],\n",
       " [u'COMEDY_PERFORMANCE'],\n",
       " [u'CAUSE'],\n",
       " [u'FOOD'],\n",
       " [u'FILM'],\n",
       " [u'THEATER', u'COMEDY_PERFORMANCE'],\n",
       " [u'CAUSE'],\n",
       " [u'CRAFTS'],\n",
       " [u'SPORTS'],\n",
       " [u'COMEDY_PERFORMANCE'],\n",
       " [u'MUSIC', u'CAUSE', u'ART'],\n",
       " [u'ART', u'CAUSE'],\n",
       " [u'COMEDY_PERFORMANCE'],\n",
       " [u'WELLNESS', u'CAUSE'],\n",
       " [u'DANCE'],\n",
       " [u'FILM'],\n",
       " [u'CAUSE'],\n",
       " [u'FILM'],\n",
       " [u'FILM'],\n",
       " [u'MUSIC'],\n",
       " [u'FILM'],\n",
       " [u'ART'],\n",
       " [u'THEATER'],\n",
       " [u'FILM'],\n",
       " [u'DANCE'],\n",
       " [u'MEETUP', u'CAUSE'],\n",
       " [u'CAUSE'],\n",
       " [u'SPORTS'],\n",
       " [u'DANCE'],\n",
       " [u'FOOD'],\n",
       " [u'CAUSE'],\n",
       " [u'FITNESS'],\n",
       " [u'CAUSE'],\n",
       " [u'CAUSE'],\n",
       " [u'DANCE', u'MUSIC'],\n",
       " [u'THEATER'],\n",
       " [u'ART'],\n",
       " [u'FILM'],\n",
       " [u'PARTY'],\n",
       " [u'NETWORKING', u'CAUSE'],\n",
       " [u'MUSIC'],\n",
       " [u'ART'],\n",
       " [u'RELIGION'],\n",
       " [u'WELLNESS'],\n",
       " [u'FOOD'],\n",
       " [u'HEALTH'],\n",
       " [u'MUSIC'],\n",
       " [u'ART'],\n",
       " [u'COMEDY_PERFORMANCE'],\n",
       " [u'THEATER'],\n",
       " [u'CAUSE'],\n",
       " [u'FILM'],\n",
       " [u'HEALTH'],\n",
       " [u'FILM'],\n",
       " [u'LITERATURE', u'CAUSE'],\n",
       " [u'SPORTS'],\n",
       " [u'HEALTH'],\n",
       " [u'MUSIC'],\n",
       " [u'CAUSE'],\n",
       " [u'THEATER'],\n",
       " [u'PARTY', u'CAUSE'],\n",
       " [u'FOOD'],\n",
       " [u'CAUSE'],\n",
       " [u'FILM'],\n",
       " [u'NETWORKING', u'FOOD', u'MEETUP'],\n",
       " [u'SPORTS'],\n",
       " [u'CAUSE'],\n",
       " [u'SHOPPING', u'CAUSE'],\n",
       " [u'CAUSE'],\n",
       " [u'FILM'],\n",
       " [u'MUSIC'],\n",
       " [u'CAUSE'],\n",
       " [u'COMEDY_PERFORMANCE'],\n",
       " [u'THEATER'],\n",
       " [u'FOOD'],\n",
       " [u'NETWORKING'],\n",
       " [u'MUSIC'],\n",
       " [u'HEALTH', u'CAUSE'],\n",
       " [u'CAUSE'],\n",
       " [u'COMEDY_PERFORMANCE'],\n",
       " [u'MUSIC'],\n",
       " [u'NETWORKING'],\n",
       " [u'FITNESS'],\n",
       " [u'CAUSE', u'SPORTS'],\n",
       " [u'THEATER'],\n",
       " [u'DANCE', u'CAUSE'],\n",
       " [u'CAUSE'],\n",
       " [u'PARTY', u'CAUSE'],\n",
       " [u'NETWORKING'],\n",
       " [u'FILM'],\n",
       " [u'CAUSE'],\n",
       " [u'PARTY', u'COMEDY_PERFORMANCE'],\n",
       " [u'THEATER'],\n",
       " [u'MUSIC'],\n",
       " [u'ART'],\n",
       " [u'SPORTS'],\n",
       " [u'RELIGION', u'CAUSE'],\n",
       " [u'CAUSE'],\n",
       " [u'PARTY'],\n",
       " [u'WELLNESS'],\n",
       " [u'THEATER'],\n",
       " [u'SPORTS'],\n",
       " [u'THEATER'],\n",
       " [u'RELIGION', u'FOOD'],\n",
       " [u'MEETUP', u'CAUSE'],\n",
       " [u'ART'],\n",
       " [u'THEATER'],\n",
       " [u'FILM'],\n",
       " [u'MUSIC', u'ART'],\n",
       " [u'COMEDY_PERFORMANCE'],\n",
       " [u'FILM'],\n",
       " [u'MEETUP'],\n",
       " [u'CAUSE'],\n",
       " [u'THEATER'],\n",
       " [u'FILM'],\n",
       " [u'PARTY'],\n",
       " [u'MEETUP', u'CAUSE'],\n",
       " [u'THEATER'],\n",
       " [u'CAUSE'],\n",
       " [u'THEATER'],\n",
       " [u'THEATER', u'MUSIC'],\n",
       " [u'THEATER'],\n",
       " [u'FOOD'],\n",
       " [u'FOOD'],\n",
       " [u'FILM'],\n",
       " [u'NETWORKING'],\n",
       " [u'FILM'],\n",
       " [u'DANCE'],\n",
       " [u'FILM'],\n",
       " [u'CAUSE'],\n",
       " [u'ART'],\n",
       " [u'THEATER'],\n",
       " [u'PARTY', u'FOOD'],\n",
       " [u'FOOD'],\n",
       " [u'THEATER'],\n",
       " [u'FILM'],\n",
       " [u'NETWORKING'],\n",
       " [u'MEETUP', u'MUSIC'],\n",
       " [u'COMEDY_PERFORMANCE'],\n",
       " [u'RELIGION', u'FOOD'],\n",
       " [u'DANCE'],\n",
       " [u'FOOD'],\n",
       " [u'FILM'],\n",
       " [u'THEATER'],\n",
       " [u'DANCE'],\n",
       " [u'MUSIC'],\n",
       " [u'FOOD'],\n",
       " [u'FILM'],\n",
       " [u'CAUSE'],\n",
       " [u'MUSIC'],\n",
       " [u'THEATER'],\n",
       " [u'FILM'],\n",
       " [u'FITNESS', u'WELLNESS'],\n",
       " [u'MUSIC'],\n",
       " [u'FOOD'],\n",
       " [u'THEATER', u'MUSIC'],\n",
       " [u'CAUSE'],\n",
       " [u'THEATER'],\n",
       " [u'NETWORKING'],\n",
       " [u'SPORTS'],\n",
       " [u'MUSIC'],\n",
       " [u'HEALTH'],\n",
       " [u'DANCE'],\n",
       " [u'FOOD'],\n",
       " [u'COMEDY_PERFORMANCE'],\n",
       " [u'COMEDY_PERFORMANCE'],\n",
       " [u'FITNESS'],\n",
       " [u'COMEDY_PERFORMANCE'],\n",
       " [u'CAUSE'],\n",
       " [u'SPORTS', u'NETWORKING', u'FOOD'],\n",
       " [u'SPORTS', u'THEATER'],\n",
       " [u'COMEDY_PERFORMANCE'],\n",
       " [u'CAUSE'],\n",
       " [u'CAUSE'],\n",
       " [u'RELIGION'],\n",
       " [u'PARTY'],\n",
       " [u'THEATER'],\n",
       " [u'THEATER', u'FILM'],\n",
       " [u'MUSIC'],\n",
       " [u'FOOD'],\n",
       " [u'FILM'],\n",
       " [u'MEETUP'],\n",
       " [u'FILM'],\n",
       " [u'CONFERENCE', u'CAUSE'],\n",
       " [u'THEATER'],\n",
       " [u'THEATER'],\n",
       " [u'CAUSE'],\n",
       " [u'CAUSE'],\n",
       " [u'FOOD', u'CAUSE'],\n",
       " [u'CAUSE'],\n",
       " [u'MUSIC'],\n",
       " [u'MUSIC'],\n",
       " [u'FILM'],\n",
       " [u'FOOD'],\n",
       " [u'PARTY'],\n",
       " [u'MUSIC'],\n",
       " [u'MUSIC'],\n",
       " [u'MUSIC'],\n",
       " [u'FILM'],\n",
       " [u'CONFERENCE', u'CAUSE'],\n",
       " [u'COMEDY_PERFORMANCE'],\n",
       " [u'CAUSE'],\n",
       " [u'HEALTH'],\n",
       " [u'MEETUP'],\n",
       " [u'MEETUP', u'MUSIC'],\n",
       " [u'ART', u'MUSIC'],\n",
       " [u'FILM'],\n",
       " [u'ART'],\n",
       " [u'CAUSE', u'MUSIC'],\n",
       " [u'MEETUP'],\n",
       " [u'FOOD'],\n",
       " [u'THEATER'],\n",
       " [u'FILM'],\n",
       " [u'THEATER'],\n",
       " [u'MUSIC'],\n",
       " [u'CAUSE'],\n",
       " [u'NETWORKING'],\n",
       " [u'THEATER'],\n",
       " [u'FILM'],\n",
       " [u'THEATER', u'FILM'],\n",
       " [u'ART'],\n",
       " [u'SPORTS'],\n",
       " [u'THEATER'],\n",
       " [u'FILM'],\n",
       " [u'HEALTH'],\n",
       " [u'NETWORKING'],\n",
       " [u'SPORTS'],\n",
       " [u'SPORTS'],\n",
       " [u'ART', u'CAUSE'],\n",
       " [u'MEETUP'],\n",
       " [u'NETWORKING'],\n",
       " [u'NETWORKING'],\n",
       " [u'SPORTS'],\n",
       " [u'CAUSE'],\n",
       " [u'MUSIC'],\n",
       " [u'DANCE'],\n",
       " [u'PARTY', u'FOOD'],\n",
       " [u'NETWORKING'],\n",
       " [u'MUSIC'],\n",
       " [u'FOOD'],\n",
       " [u'MUSIC'],\n",
       " [u'CAUSE'],\n",
       " [u'MUSIC'],\n",
       " [u'THEATER', u'FILM'],\n",
       " [u'SPORTS'],\n",
       " [u'CAUSE'],\n",
       " [u'ART'],\n",
       " [u'PARTY'],\n",
       " [u'COMEDY_PERFORMANCE'],\n",
       " [u'THEATER'],\n",
       " [u'SPORTS', u'FOOD'],\n",
       " [u'FILM'],\n",
       " [u'FOOD'],\n",
       " [u'SPORTS'],\n",
       " [u'GAMES'],\n",
       " [u'PARTY'],\n",
       " [u'MEETUP'],\n",
       " [u'CAUSE'],\n",
       " [u'FILM'],\n",
       " [u'FOOD', u'CAUSE'],\n",
       " [u'MEETUP'],\n",
       " [u'DRINK', u'FOOD'],\n",
       " [u'PARTY'],\n",
       " [u'FOOD'],\n",
       " [u'DANCE'],\n",
       " [u'SPORTS'],\n",
       " [u'MUSIC'],\n",
       " [u'ART'],\n",
       " [u'FOOD', u'NETWORKING'],\n",
       " [u'RELIGION'],\n",
       " [u'FILM'],\n",
       " [u'CAUSE'],\n",
       " [u'COMEDY_PERFORMANCE'],\n",
       " [u'THEATER'],\n",
       " [u'MUSIC'],\n",
       " [u'THEATER'],\n",
       " [u'THEATER'],\n",
       " [u'MEETUP', u'FILM'],\n",
       " [u'FILM'],\n",
       " [u'MUSIC', u'SPORTS'],\n",
       " [u'ART'],\n",
       " [u'CAUSE'],\n",
       " [u'ART'],\n",
       " [u'MEETUP', u'MUSIC'],\n",
       " [u'ART'],\n",
       " [u'FOOD', u'CAUSE'],\n",
       " [u'NETWORKING'],\n",
       " [u'MUSIC'],\n",
       " [u'CAUSE'],\n",
       " [u'CAUSE'],\n",
       " [u'COMEDY_PERFORMANCE'],\n",
       " [u'MUSIC'],\n",
       " [u'CAUSE'],\n",
       " [u'FILM', u'MUSIC'],\n",
       " [u'ART'],\n",
       " [u'WELLNESS'],\n",
       " [u'THEATER'],\n",
       " [u'MEETUP'],\n",
       " [u'NETWORKING'],\n",
       " [u'THEATER'],\n",
       " [u'CONFERENCE'],\n",
       " [u'COMEDY_PERFORMANCE'],\n",
       " [u'THEATER'],\n",
       " [u'RELIGION'],\n",
       " [u'CAUSE'],\n",
       " [u'CAUSE'],\n",
       " [u'FOOD', u'CAUSE'],\n",
       " [u'MUSIC'],\n",
       " [u'DANCE'],\n",
       " [u'MUSIC'],\n",
       " [u'SPORTS'],\n",
       " [u'FOOD'],\n",
       " [u'SPORTS'],\n",
       " [u'ART', u'FILM'],\n",
       " [u'FOOD'],\n",
       " [u'FITNESS'],\n",
       " [u'MEETUP', u'MUSIC'],\n",
       " [u'THEATER'],\n",
       " [u'NETWORKING'],\n",
       " [u'THEATER'],\n",
       " [u'THEATER'],\n",
       " [u'CAUSE'],\n",
       " [u'THEATER'],\n",
       " [u'THEATER'],\n",
       " [u'FILM'],\n",
       " [u'MUSIC', u'ART'],\n",
       " [u'MUSIC'],\n",
       " [u'MUSIC'],\n",
       " [u'MUSIC'],\n",
       " [u'FOOD'],\n",
       " [u'CAUSE'],\n",
       " [u'CAUSE'],\n",
       " [u'CAUSE'],\n",
       " [u'FITNESS', u'MUSIC'],\n",
       " [u'MEETUP'],\n",
       " [u'CAUSE'],\n",
       " [u'SPORTS'],\n",
       " [u'THEATER'],\n",
       " [u'SPORTS'],\n",
       " [u'FILM'],\n",
       " [u'THEATER'],\n",
       " [u'MUSIC'],\n",
       " [u'ART'],\n",
       " [u'THEATER'],\n",
       " [u'MUSIC'],\n",
       " [u'SPORTS'],\n",
       " [u'MEETUP', u'NETWORKING'],\n",
       " [u'THEATER'],\n",
       " [u'CAUSE', u'MUSIC'],\n",
       " [u'DANCE'],\n",
       " [u'FITNESS'],\n",
       " [u'HEALTH'],\n",
       " [u'MEETUP'],\n",
       " [u'NETWORKING'],\n",
       " [u'THEATER'],\n",
       " [u'CAUSE', u'FOOD'],\n",
       " [u'HEALTH', u'CAUSE', u'SPORTS'],\n",
       " [u'CAUSE'],\n",
       " [u'MUSIC'],\n",
       " [u'CAUSE'],\n",
       " [u'THEATER'],\n",
       " [u'ART'],\n",
       " [u'PARTY'],\n",
       " [u'FOOD', u'CAUSE'],\n",
       " [u'MEETUP'],\n",
       " [u'CAUSE'],\n",
       " [u'MEETUP'],\n",
       " [u'THEATER', u'MUSIC'],\n",
       " [u'CAUSE'],\n",
       " [u'FOOD'],\n",
       " [u'HEALTH'],\n",
       " [u'NETWORKING', u'CAUSE'],\n",
       " [u'THEATER'],\n",
       " [u'MUSIC', u'WELLNESS', u'ART'],\n",
       " [u'COMEDY_PERFORMANCE'],\n",
       " [u'NETWORKING'],\n",
       " [u'CAUSE', u'FOOD'],\n",
       " [u'SPORTS'],\n",
       " [u'FILM'],\n",
       " [u'THEATER', u'FILM'],\n",
       " [u'FOOD'],\n",
       " [u'CONFERENCE'],\n",
       " [u'RELIGION', u'FOOD'],\n",
       " [u'SPORTS'],\n",
       " [u'CAUSE'],\n",
       " [u'MUSIC'],\n",
       " [u'PARTY'],\n",
       " [u'MEETUP'],\n",
       " [u'MUSIC', u'ART'],\n",
       " [u'FILM', u'THEATER'],\n",
       " [u'ART'],\n",
       " [u'LITERATURE', u'THEATER'],\n",
       " [u'NETWORKING'],\n",
       " [u'SPORTS'],\n",
       " [u'NETWORKING'],\n",
       " [u'MUSIC'],\n",
       " [u'SPORTS'],\n",
       " [u'ART'],\n",
       " [u'THEATER', u'MUSIC'],\n",
       " [u'ART'],\n",
       " [u'MUSIC'],\n",
       " [u'THEATER'],\n",
       " [u'DANCE', u'ART'],\n",
       " [u'FILM'],\n",
       " [u'PARTY', u'MUSIC'],\n",
       " [u'MEETUP'],\n",
       " [u'SPORTS'],\n",
       " [u'COMEDY_PERFORMANCE'],\n",
       " [u'CONFERENCE', u'CAUSE'],\n",
       " [u'THEATER'],\n",
       " [u'FILM'],\n",
       " [u'CAUSE'],\n",
       " [u'FOOD'],\n",
       " [u'MEETUP'],\n",
       " [u'FOOD', u'MEETUP'],\n",
       " [u'LITERATURE'],\n",
       " [u'SPORTS'],\n",
       " [u'SPORTS', u'MUSIC'],\n",
       " [u'NETWORKING'],\n",
       " [u'CAUSE'],\n",
       " [u'CAUSE'],\n",
       " [u'FILM'],\n",
       " [u'CONFERENCE', u'ART'],\n",
       " [u'DANCE', u'CAUSE'],\n",
       " [u'CAUSE'],\n",
       " [u'MUSIC'],\n",
       " [u'FOOD', u'CAUSE'],\n",
       " [u'CAUSE'],\n",
       " [u'THEATER'],\n",
       " [u'FOOD', u'CAUSE'],\n",
       " [u'CAUSE'],\n",
       " [u'NETWORKING', u'MUSIC'],\n",
       " [u'FOOD'],\n",
       " [u'THEATER'],\n",
       " [u'RELIGION', u'CAUSE'],\n",
       " [u'MUSIC'],\n",
       " [u'THEATER'],\n",
       " [u'FOOD'],\n",
       " [u'NETWORKING'],\n",
       " ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X['description'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X[2:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
