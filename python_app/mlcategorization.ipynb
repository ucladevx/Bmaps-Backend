{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "92f7983f-ec8a-41dd-987a-913b1754d3a4"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jfuentes/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /Users/jfuentes/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /Users/jfuentes/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/jfuentes/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('perluniprops')\n",
    "nltk.download('nonbreaking_prefixes')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "e9a9474a-3d31-4794-be29-34bad530e8ea"
    }
   },
   "outputs": [],
   "source": [
    "from ourDb import events_collection, total_events_collection, events_ml_collection\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from nltk.tokenize.moses import MosesTokenizer\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from gensim.models import Phrases\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "b9cffde6-aea9-4d79-914e-6958234c679b"
    }
   },
   "outputs": [],
   "source": [
    "class PreprocessText:\n",
    "    def __init__(self, categorizedEvents):\n",
    "        \"\"\"categorizedEvents should be a list of dictionaries each corresponding to an event\n",
    "            X is the tokenized preprocessed text\n",
    "            Y is the corresponding categories\n",
    "            phraseMl is the phrase model that can further trained and used\n",
    "            phrases is a list of all the phrases identified\"\"\"\n",
    "        self.X = []\n",
    "        self.Y = []\n",
    "        tokenizer = MosesTokenizer() #tokenizers are basically an advanced split\n",
    "        for e in categorizedEvents:\n",
    "            text = e[u'name'] + \" \" + e[u'description']\n",
    "            text = tokenizer.tokenize(text)\n",
    "            text = self.preprocess(text)\n",
    "            self.X.append(text)\n",
    "            self.Y.append(e[u'category'])\n",
    "        self.phraseMl = Phrases(self.X, min_count=3) #learn ml model for phrase\n",
    "        self.X = list(self.phraseMl[self.X]) #use ml model for phrases\n",
    "#         self.X = list(self.phraseMl[self.X]) #get triples\n",
    "        self.phrases = phrases = set([w for doc in self.X for w in doc if '_' in w])\n",
    "        \n",
    "    def matchNotX(self, strg, search=re.compile(r'[^!#$%&()*+,-./:;<=>?@\\\\^_}|{~0123456789]').search):\n",
    "        \"\"\"make sure word has something than punctuation\"\"\"\n",
    "        return bool(search(strg)) #make sure word has something other than punctuation\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Remove all useless words and punct, make lowercase\"\"\"\n",
    "        stoplist = set('for a of the and to in . / '.split())\n",
    "        stoplist = set(nltk.corpus.stopwords.words('english')) | stoplist | set(string.punctuation)\n",
    "        return [word.strip(string.punctuation).lower() for word in text if word not in stoplist and self.matchNotX(word)]    \n",
    "        \n",
    "    def topBigrams(self, texts, n, tri=False):\n",
    "        \"\"\"Other method of getting phrases, currently unused because phrases can be further trained(online) and saved\"\"\"\n",
    "        flatTexts = []\n",
    "        for text in texts:\n",
    "            for word in text:\n",
    "                flatTexts.append(word)\n",
    "        bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "        trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "        topAnswers = []\n",
    "        if tri:\n",
    "            finder = nltk.collocations.TrigramCollocationFinder.from_words(flatTexts)\n",
    "            finder.apply_freq_filter(7)\n",
    "            return finder.nbest(trigram_measures.pmi, n)\n",
    "        else:\n",
    "            finder = nltk.collocations.BigramCollocationFinder.from_words(flatTexts)\n",
    "            finder.apply_freq_filter(7)\n",
    "            return finder.nbest(bigram_measures.pmi, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "nbpresent": {
     "id": "90872305-5f7a-4d08-98bc-acd0c8198395"
    }
   },
   "outputs": [],
   "source": [
    "def gatherCategorizedEvents():\n",
    "    allCategorizedEvents = []\n",
    "    allEvents = events_ml_collection.find({}, {\"category\": 1, \"description\": 1, \"name\": 1, \"_id\": 0})\n",
    "    count = 0\n",
    "    for e in allEvents:\n",
    "        count += 1\n",
    "        if 'category' in e and 'description' in e and 'name' in e:\n",
    "            allCategorizedEvents.append(e)\n",
    "    modernEvents = reduceCategories(allCategorizedEvents)\n",
    "    print count, \"total events, learning from the\", len(modernEvents), \"categorized events\"\n",
    "    return modernEvents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def someCurrentCategories():\n",
    "    \"\"\"Looks at current events for the categories list, to be used if Facebook changes its events in the future\"\"\"\n",
    "    allCategorizedEvents = []\n",
    "    allEvents = events_collection.find({}, {\"category\": 1, \"description\": 1, \"name\": 1, \"_id\": 0})\n",
    "    for e in allEvents:\n",
    "        if 'category' in e and 'description' in e and 'name' in e:\n",
    "            allCategorizedEvents.append(e)\n",
    "    skTarget = [e['category'] for e in allCategorizedEvents]\n",
    "    count = sorted(list(set(skTarget)))\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduceCategories(events):\n",
    "    \"\"\"OTHER will discarded from the training data\"\"\"\n",
    "    categoryMapping = {\n",
    "        u'BOOK': u'LITERATURE',\n",
    "        u'COMEDY': u'COMEDY_PERFORMANCE',\n",
    "        u'CLASS': u'OTHER',\n",
    "        u'DINING': u'FOOD',\n",
    "        u'FAMILY': u'OTHER',\n",
    "        u'FESTIVAL': u'PARTY',\n",
    "        u'FOOD_TASTING': u'FOOD',\n",
    "        u'FUNDRAISER': u'CAUSE',\n",
    "        u'LECTURE': u'OTHER',\n",
    "        u'MOVIE': u'FILM',\n",
    "        u'NEIGHBORHOOD': u'OTHER',\n",
    "        u'NIGHTLIFE': u'OTHER',\n",
    "        u'RELIGIOUS': u'RELIGION',\n",
    "        u'VOLUNTEERING': u'CAUSE',\n",
    "        u'WORKSHOP': u'OTHER'\n",
    "    }\n",
    "    \n",
    "    for e in events:\n",
    "        if e in categoryMapping:\n",
    "            \n",
    "    \n",
    "    \n",
    "    return events\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "nbpresent": {
     "id": "fb083146-ae02-494f-9da7-8ae1a483ef92"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5819 total events, learning from the 2907 categorized events\n"
     ]
    }
   ],
   "source": [
    "curListOfCategories = [u'ART', u'CAUSE', u'COMEDY_PERFORMANCE', u'DANCE', u'DRINKS', u'FILM', u'FITNESS', u'FOOD',\n",
    "                       u'GAMES', u'GARDENING', u'HEALTH', u'LITERATURE', u'MEETUP', u'MUSIC', u'NETWORKING', u'PARTY',\n",
    "                       u'RELIGION', u'SHOPPING', u'SPORTS', u'THEATER', u'WELLNESS']\n",
    "X = gatherCategorizedEvents()\n",
    "skText = [e['name']+' '+e['description'] for e in X]\n",
    "skTarget = [e['category'] for e in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'ART', u'CAUSE', u'COMEDY_PERFORMANCE', u'CONFERENCE', u'DANCE', u'FILM', u'FITNESS', u'FOOD', u'GAMES', u'GARDENING', u'HEALTH', u'LITERATURE', u'MEETUP', u'MUSIC', u'NETWORKING', u'PARTY', u'SPORTS', u'THEATER', u'WELLNESS']\n"
     ]
    }
   ],
   "source": [
    "someCurrentCategories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'ART',\n",
       " u'BOOK',\n",
       " u'CAUSE',\n",
       " u'CLASS',\n",
       " u'COMEDY',\n",
       " u'COMEDY_PERFORMANCE',\n",
       " u'CONFERENCE',\n",
       " u'CRAFTS',\n",
       " u'DANCE',\n",
       " u'DINING',\n",
       " u'DRINK',\n",
       " u'FAMILY',\n",
       " u'FESTIVAL',\n",
       " u'FILM',\n",
       " u'FITNESS',\n",
       " u'FOOD',\n",
       " u'FOOD_TASTING',\n",
       " u'FUNDRAISER',\n",
       " u'GAMES',\n",
       " u'GARDENING',\n",
       " u'HEALTH',\n",
       " u'LECTURE',\n",
       " u'LITERATURE',\n",
       " u'MEETUP',\n",
       " u'MOVIE',\n",
       " u'MUSIC',\n",
       " u'NEIGHBORHOOD',\n",
       " u'NETWORKING',\n",
       " u'NIGHTLIFE',\n",
       " u'OTHER',\n",
       " u'PARTY',\n",
       " u'RELIGIOUS',\n",
       " u'SHOPPING',\n",
       " u'SPORTS',\n",
       " u'THEATER',\n",
       " u'VOLUNTEERING',\n",
       " u'WELLNESS',\n",
       " u'WORKSHOP']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(set(skTarget)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEIGHBORHOOD\n",
      "MOVIE\n",
      "FUNDRAISER\n",
      "GAMES\n",
      "NIGHTLIFE\n",
      "FITNESS\n",
      "LECTURE\n",
      "FOOD_TASTING\n",
      "FILM\n",
      "CONFERENCE\n",
      "SHOPPING\n",
      "NETWORKING\n",
      "VOLUNTEERING\n",
      "DANCE\n",
      "WELLNESS\n",
      "DRINK\n",
      "MEETUP\n",
      "DINING\n",
      "WORKSHOP\n",
      "COMEDY\n",
      "RELIGIOUS\n",
      "CLASS\n",
      "FOOD\n",
      "LITERATURE\n",
      "THEATER\n",
      "FAMILY\n",
      "COMEDY_PERFORMANCE\n",
      "GARDENING\n",
      "OTHER\n",
      "MUSIC\n",
      "PARTY\n",
      "CAUSE\n",
      "ART\n",
      "FESTIVAL\n",
      "SPORTS\n",
      "BOOK\n",
      "HEALTH\n",
      "CRAFTS\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "for i in Counter(skTarget).keys():\n",
    "    print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "def train(classifier, X, y, trails=25):\n",
    "    scores = np.zeros(trails)\n",
    "    for i in tqdm(range(0, trails)):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=i)\n",
    "\n",
    "        classifier.fit(X_train, y_train)\n",
    "        scores[i] = (classifier.score(X_test, y_test))\n",
    "    print \"Average Accuracy over %d trials: %s\" % (trails, np.mean(scores))\n",
    "    classifier.fit(X, y)\n",
    "    return classifier\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictSingle(vectorizer, classifier, x):\n",
    "    print(x)\n",
    "    x = vectorizer.transform(x)\n",
    "    y_pred = classifier.predict(x)\n",
    "    print(y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "nbpresent": {
     "id": "2ce54c9a-1a7a-4d84-9972-922c9aa4db15"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:12<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy over 25 trials: 0.39807427785419536\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    " \n",
    "trial1 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', MultinomialNB()),\n",
    "])\n",
    " \n",
    "nbModel = train(trial1, skText, skTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:12<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy over 25 trials: 0.5727647867950482\n"
     ]
    }
   ],
   "source": [
    "trial2 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', MultinomialNB(alpha=0.05)),\n",
    "])\n",
    "\n",
    "nbModel = train(trial2, skText, skTarget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.5709491059147179 without stoplist \n",
    "\n",
    "0.579697386519945 with my stoplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 32.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy over 25 trials: 0.582283356258597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "stoplist = 'for a of the and to in ucla . / '.split()\n",
    "stoplist = nltk.corpus.stopwords.words('english') + stoplist + string.punctuation.split()\n",
    "\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# tokenize and build vocab\n",
    "X = vectorizer.fit_transform(skText)\n",
    "\n",
    "# print(vectorizer.vocabulary_)\n",
    "# print(vectorizer.idf_)\n",
    "\n",
    "nbModel = MultinomialNB(alpha=0.05)\n",
    "nbModel = train(nbModel, X, skTarget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.5689133425034388 Tried PorterSteemer\n",
    "\n",
    "0.5702338376891334 portersteemer with matchNotX\n",
    "\n",
    "0.5700137551581843 just snowball stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 35.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy over 25 trials: 0.5712792297111416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def matchNotX(strg, search=re.compile(r'[^!#$%&()*+,-./:;<=>?@\\\\^_}|{~0123456789]').search):\n",
    "    \"\"\"make sure word has something than punctuation\"\"\"\n",
    "    return bool(search(strg)) #make sure word has something other than punctuation\n",
    "\n",
    "def stemming_tokenizer(text):\n",
    "    stemmer = SnowballStemmer('english') #used to remove plural version of words\n",
    "    tokenizer = word_tokenize\n",
    "    return [stemmer.stem(w) for w in tokenizer(text) if matchNotX(w)]\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=stemming_tokenizer, stop_words='english') \n",
    "X = vectorizer.fit_transform(skText)\n",
    "nbModel = MultinomialNB(alpha=0.05)\n",
    "nbModel = train(nbModel, X, skTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Travis Wall\\'s Shaping Sound Under the Artistic Direction of Emmy Award Winner, TRAVIS WALL, and co-created with NICK LAZZARINI, TEDDY FORANCE and KYLE ROBINSON, SHAPING SOUND is an electrifying mash-up of dance styles and musical genres brought fully to life on stage by a dynamic company of contemporary dancers.\\n\\nThrough his Emmy Award winning work on \"So You Think You Can Dance,\" Travis Wall has established himself as one of America\\'s favorite choreographers. His artistic direction of SHAPING SOUND has already produced one spell-binding show in Dance Reimagined, which has captivated audiences all across North America. Now, Travis and SHAPING SOUND are back with their brand new show, After the Curtain. These visual musicians continue to dazzle audiences as they tell the story of a man fighting to find his creative voice after the death of his one true love. Heart-wrenching, breath-taking and ultimately uplifting, this is one dance show that audiences will remember forever.']\n",
      "[u'THEATER']\n"
     ]
    }
   ],
   "source": [
    "predictSingle(vectorizer, nbModel, skText[5:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'ART', u'BOOK', u'CAUSE', u'CLASS', u'COMEDY',\n",
       "       u'COMEDY_PERFORMANCE', u'CONFERENCE', u'CRAFTS', u'DANCE',\n",
       "       u'DINING', u'DRINK', u'FAMILY', u'FESTIVAL', u'FILM', u'FITNESS',\n",
       "       u'FOOD', u'FOOD_TASTING', u'FUNDRAISER', u'GAMES', u'GARDENING',\n",
       "       u'HEALTH', u'LECTURE', u'LITERATURE', u'MEETUP', u'MOVIE',\n",
       "       u'MUSIC', u'NEIGHBORHOOD', u'NETWORKING', u'NIGHTLIFE', u'OTHER',\n",
       "       u'PARTY', u'RELIGIOUS', u'SHOPPING', u'SPORTS', u'THEATER',\n",
       "       u'VOLUNTEERING', u'WELLNESS', u'WORKSHOP'], dtype='<U18')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2907x19447 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 167832 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = vectorizer.build_tokenizer()\n",
    "\n",
    "for w in vectorizer.vocabulary_:\n",
    "    if not matchNotX(w):\n",
    "        print(w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
