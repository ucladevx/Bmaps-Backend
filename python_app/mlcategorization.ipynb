{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "nbpresent": {
     "id": "92f7983f-ec8a-41dd-987a-913b1754d3a4"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 8] nodename\n",
      "[nltk_data]     nor servname provided, or not known>\n",
      "[nltk_data] Error loading perluniprops: <urlopen error [Errno 8]\n",
      "[nltk_data]     nodename nor servname provided, or not known>\n",
      "[nltk_data] Error loading nonbreaking_prefixes: <urlopen error [Errno\n",
      "[nltk_data]     8] nodename nor servname provided, or not known>\n",
      "[nltk_data] Error loading punkt: <urlopen error [Errno 8] nodename nor\n",
      "[nltk_data]     servname provided, or not known>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('perluniprops')\n",
    "nltk.download('nonbreaking_prefixes')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "nbpresent": {
     "id": "e9a9474a-3d31-4794-be29-34bad530e8ea"
    }
   },
   "outputs": [],
   "source": [
    "from ourDb import events_collection, total_events_collection, events_ml_collection\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from nltk.tokenize.moses import MosesTokenizer\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from gensim.models import Phrases\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "b9cffde6-aea9-4d79-914e-6958234c679b"
    }
   },
   "outputs": [],
   "source": [
    "class PreprocessText:\n",
    "    def __init__(self, categorizedEvents):\n",
    "        \"\"\"categorizedEvents should be a list of dictionaries each corresponding to an event\n",
    "            X is the tokenized preprocessed text\n",
    "            Y is the corresponding categories\n",
    "            phraseMl is the phrase model that can further trained and used\n",
    "            phrases is a list of all the phrases identified\"\"\"\n",
    "        self.X = []\n",
    "        self.Y = []\n",
    "        tokenizer = MosesTokenizer() #tokenizers are basically an advanced split\n",
    "        for e in categorizedEvents:\n",
    "            text = e[u'name'] + \" \" + e[u'description']\n",
    "            text = tokenizer.tokenize(text)\n",
    "            text = self.preprocess(text)\n",
    "            self.X.append(text)\n",
    "            self.Y.append(e[u'category'])\n",
    "        self.phraseMl = Phrases(self.X, min_count=3) #learn ml model for phrase\n",
    "        self.X = list(self.phraseMl[self.X]) #use ml model for phrases\n",
    "#         self.X = list(self.phraseMl[self.X]) #get triples\n",
    "        self.phrases = phrases = set([w for doc in self.X for w in doc if '_' in w])\n",
    "        \n",
    "    def matchNotX(self, strg, search=re.compile(r'[^!#$%&()*+,-./:;<=>?@\\\\^_}|{~0123456789]').search):\n",
    "        \"\"\"make sure word has something than punctuation\"\"\"\n",
    "        return bool(search(strg)) #make sure word has something other than punctuation\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Remove all useless words and punct, make lowercase\"\"\"\n",
    "        stoplist = set('for a of the and to in . / '.split())\n",
    "        stoplist = set(nltk.corpus.stopwords.words('english')) | stoplist | set(string.punctuation)\n",
    "        return [word.strip(string.punctuation).lower() for word in text if word not in stoplist and self.matchNotX(word)]    \n",
    "        \n",
    "    def topBigrams(self, texts, n, tri=False):\n",
    "        \"\"\"Other method of getting phrases, currently unused because phrases can be further trained(online) and saved\"\"\"\n",
    "        flatTexts = []\n",
    "        for text in texts:\n",
    "            for word in text:\n",
    "                flatTexts.append(word)\n",
    "        bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "        trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "        topAnswers = []\n",
    "        if tri:\n",
    "            finder = nltk.collocations.TrigramCollocationFinder.from_words(flatTexts)\n",
    "            finder.apply_freq_filter(7)\n",
    "            return finder.nbest(trigram_measures.pmi, n)\n",
    "        else:\n",
    "            finder = nltk.collocations.BigramCollocationFinder.from_words(flatTexts)\n",
    "            finder.apply_freq_filter(7)\n",
    "            return finder.nbest(bigram_measures.pmi, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "nbpresent": {
     "id": "90872305-5f7a-4d08-98bc-acd0c8198395"
    }
   },
   "outputs": [],
   "source": [
    "def gatherCategorizedEvents():\n",
    "    allCategorizedEvents = []\n",
    "    allEvents = events_ml_collection.find({}, {\"category\": 1, \"description\": 1, \"name\": 1, \"_id\": 0})\n",
    "    count = 0\n",
    "    for e in allEvents:\n",
    "        count += 1\n",
    "        if 'category' in e and 'description' in e and 'name' in e:\n",
    "            allCategorizedEvents.append(e)\n",
    "    modernEvents = reduceCategories(allCategorizedEvents)\n",
    "    print count, \"total events, learning from the\", len(modernEvents), \"well categorized events\"\n",
    "    return modernEvents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def someCurrentCategories():\n",
    "    \"\"\"Looks at current events for the categories list, to be used if Facebook changes its events in the future\"\"\"\n",
    "    allCategorizedEvents = []\n",
    "    allEvents = events_collection.find({}, {\"category\": 1, \"description\": 1, \"name\": 1, \"_id\": 0})\n",
    "    for e in allEvents:\n",
    "        if 'category' in e and 'description' in e and 'name' in e:\n",
    "            allCategorizedEvents.append(e)\n",
    "    skTarget = [e['category'] for e in allCategorizedEvents]\n",
    "    count = sorted(list(set(skTarget)))\n",
    "    print(count)\n",
    "    \n",
    "curListOfCategories = [u'ART', u'CAUSE', u'COMEDY_PERFORMANCE', u'DANCE', u'DRINKS', u'FILM', u'FITNESS', u'FOOD',\n",
    "                       u'GAMES', u'GARDENING', u'HEALTH', u'LITERATURE', u'MEETUP', u'MUSIC', u'NETWORKING', u'PARTY',\n",
    "                       u'RELIGION', u'SHOPPING', u'SPORTS', u'THEATER', u'WELLNESS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduceCategories(events):\n",
    "    \"\"\"OTHER will discarded from the training data\"\"\"\n",
    "    categoryMapping = {\n",
    "        u'BOOK': u'LITERATURE',\n",
    "        u'COMEDY': u'COMEDY_PERFORMANCE',\n",
    "        u'CLASS': u'OTHER',\n",
    "        u'DINING': u'FOOD',\n",
    "        u'FAMILY': u'OTHER',\n",
    "        u'FESTIVAL': u'PARTY',\n",
    "        u'FOOD_TASTING': u'FOOD',\n",
    "        u'FUNDRAISER': u'CAUSE',\n",
    "        u'LECTURE': u'OTHER',\n",
    "        u'MOVIE': u'FILM',\n",
    "        u'NEIGHBORHOOD': u'OTHER',\n",
    "        u'NIGHTLIFE': u'OTHER',\n",
    "        u'RELIGIOUS': u'RELIGION',\n",
    "        u'VOLUNTEERING': u'CAUSE',\n",
    "        u'WORKSHOP': u'OTHER'\n",
    "    }\n",
    "    \n",
    "    for e in events:\n",
    "        category = e['category']\n",
    "        if category in categoryMapping:\n",
    "            e['category'] = categoryMapping[category]\n",
    "    reducedEvents = [e for e in events if e['category'] != u'OTHER']\n",
    "    return reducedEvents\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "nbpresent": {
     "id": "fb083146-ae02-494f-9da7-8ae1a483ef92"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5843 total events, learning from the 2291 well categorized events\n"
     ]
    }
   ],
   "source": [
    "X = gatherCategorizedEvents()\n",
    "skText = [e['name']+' '+e['description'] for e in X]\n",
    "skTarget = [e['category'] for e in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "def train(classifier, X, y, trails=25):\n",
    "    scores = np.zeros(trails)\n",
    "    for i in tqdm(range(0, trails)):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=i)\n",
    "\n",
    "        classifier.fit(X_train, y_train)\n",
    "        scores[i] = (classifier.score(X_test, y_test))\n",
    "    print \"Average Accuracy over %d trials: %s\" % (trails, np.mean(scores))\n",
    "    classifier.fit(X, y)\n",
    "    return classifier\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictList(vectorizer, classifier, x):\n",
    "    print(x)\n",
    "    x = vectorizer.transform(x)\n",
    "    y_pred = classifier.predict(x)\n",
    "    print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def giveProbPerCategory(vectorizer, classifier, x):\n",
    "    print(x)\n",
    "    x = vectorizer.transform(x)\n",
    "    y_pred = classifier.predict_proba(x)\n",
    "    for e_pref in y_pred:\n",
    "        newDict = {}\n",
    "        for i, y in enumerate(e_pref):\n",
    "            category = classifier.classes_[i] \n",
    "            prob = y[i]\n",
    "            \n",
    "    print(classifier.classes_)\n",
    "    print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "nbpresent": {
     "id": "2ce54c9a-1a7a-4d84-9972-922c9aa4db15"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:10<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy over 25 trials: 0.49877835951134386\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    " \n",
    "trial1 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', MultinomialNB()),\n",
    "])\n",
    " \n",
    "nbModel = train(trial1, skText, skTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:11<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy over 25 trials: 0.7001047120418847\n"
     ]
    }
   ],
   "source": [
    "trial2 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', MultinomialNB(alpha=0.05)),\n",
    "])\n",
    "\n",
    "nbModel = train(trial2, skText, skTarget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.7022687609075043 without stoplist \n",
    "\n",
    "0.7209773123909248 with my stoplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 49.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy over 25 trials: 0.7225130890052357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "stoplist = 'for a of the and to in ucla . / '.split()\n",
    "stoplist = nltk.corpus.stopwords.words('english') + stoplist + string.punctuation.split()\n",
    "\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# tokenize and build vocab\n",
    "X = vectorizer.fit_transform(skText)\n",
    "\n",
    "# print(vectorizer.vocabulary_)\n",
    "# print(vectorizer.idf_)\n",
    "\n",
    "nbModel = MultinomialNB(alpha=0.05)\n",
    "nbModel = train(nbModel, X, skTarget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.7177661431064574 portersteemer with matchNotX\n",
    "\n",
    "0.7091099476439789 just snowball stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 45.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy over 25 trials: 0.7091099476439789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def matchNotX(strg, search=re.compile(r'[^!#$%&()*+,-./:;<=>?@\\\\^_}|{~0123456789]').search):\n",
    "    \"\"\"make sure word has something than punctuation\"\"\"\n",
    "    return bool(search(strg)) #make sure word has something other than punctuation\n",
    "\n",
    "def stemming_tokenizer(text):\n",
    "    stemmer = SnowballStemmer('english') #used to remove plural version of words\n",
    "    tokenizer = word_tokenize\n",
    "    return [stemmer.stem(w) for w in tokenizer(text)]\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=stemming_tokenizer, stop_words='english') \n",
    "X = vectorizer.fit_transform(skText)\n",
    "nbModel = MultinomialNB(alpha=0.05)\n",
    "nbModel = train(nbModel, X, skTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'A Funny Thing Happened on the Way to the Gynecologic Oncology A Funny Thing Happened on the Way to the Gynecologic Oncology Unit at Memorial Sloan Kettering Cancer Center of New York City\\nSeptember 5 - October 8, 2017 \\nGil Cates Theater\\nWritten by Halley Feiffer\\nDirected by Trip Cullman\\nFeaturing Halley Feiffer, Jason Butler Harner, Eileen T\\'Kaye & JoBeth Williams\\n\\n\"Under Trip Cullman\\'s perceptive direction, Halley Feiffer\\'s distinct voice is on fine display throughout, in all its uniquely unsettling glory.\"  \\u2013 TheaterMania \\n\\nSitting bedside at Memorial Sloan Kettering has never been so entertaining. While their ailing mothers share a hospital room, Karla and Don discover truth in the old clich\\xe9 that opposites attract\\u2026and repel\\u2026and attract.']\n",
      "[u'ART' u'CAUSE' u'COMEDY_PERFORMANCE' u'CONFERENCE' u'CRAFTS' u'DANCE'\n",
      " u'DRINK' u'FILM' u'FITNESS' u'FOOD' u'GAMES' u'GARDENING' u'HEALTH'\n",
      " u'LITERATURE' u'MEETUP' u'MUSIC' u'NETWORKING' u'PARTY' u'RELIGION'\n",
      " u'SHOPPING' u'SPORTS' u'THEATER' u'WELLNESS']\n",
      "[[2.58382073e-11 6.87611292e-12 6.13523106e-12 5.40947572e-12\n",
      "  1.48632844e-13 5.96643639e-12 7.19587346e-14 4.88319002e-11\n",
      "  1.35074216e-12 2.54953256e-12 2.90287864e-13 4.14007814e-14\n",
      "  2.71432897e-12 6.43030372e-13 1.42821244e-11 3.02737376e-11\n",
      "  8.24386123e-12 3.69070874e-12 1.44392462e-12 4.84778386e-14\n",
      "  7.93620838e-13 1.00000000e+00 8.36929778e-13]]\n"
     ]
    }
   ],
   "source": [
    "giveProbPerCategory(vectorizer, nbModel, skText[5:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04\n",
      "03\n",
      "05\n",
      "09\n",
      "11\n",
      "12\n",
      "19\n",
      "18\n",
      "66\n",
      "68\n",
      "89\n",
      "98\n",
      "91\n",
      "90\n",
      "94\n",
      "96\n",
      "02\n",
      "92\n"
     ]
    }
   ],
   "source": [
    "tokenizer = vectorizer.build_tokenizer()\n",
    "\n",
    "for w in vectorizer.vocabulary_:\n",
    "    if not matchNotX(w):\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(list(set(skTarget)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
